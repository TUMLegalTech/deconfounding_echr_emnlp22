{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmnXJg-pb3Qt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
        "from transformers import BertModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
        "from transformers import BertModel\n",
        "\n",
        "\n",
        "class WordAttentionPre(torch.nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super().__init__()\n",
        "        self._device = device\n",
        "\n",
        "    def forward(self, docs, doc_lengths, sent_lengths, attention_masks, token_type_ids):\n",
        "        \"\"\"\n",
        "        :param docs: encoded document-level data; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
        "        :param doc_lengths: unpadded document lengths; LongTensor (num_docs)\n",
        "        :param sent_lengths: unpadded sentence lengths; LongTensor (num_docs, max_sent_len)\n",
        "        :param attention_masks: BERT attention masks; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
        "        :param token_type_ids: BERT token type IDs; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
        "        :return: sentences embeddings, docs permutation indices, docs batch sizes, word attention weights\n",
        "        \"\"\"\n",
        "\n",
        "        # Sort documents by decreasing order in length\n",
        "        doc_lengths, doc_perm_idx = doc_lengths.sort(dim=0, descending=True)\n",
        "        docs = docs[doc_perm_idx]\n",
        "        sent_lengths = sent_lengths[doc_perm_idx]\n",
        "        attention_masks = attention_masks[doc_perm_idx]\n",
        "        token_type_ids = token_type_ids[doc_perm_idx]\n",
        "        #print(doc_lengths, doc_perm_idx, docs, sent_lengths,attention_masks, token_type_ids)\n",
        "        \n",
        "        # Make a long batch of sentences by removing pad-sentences\n",
        "        # i.e. `docs` was of size (num_docs, padded_doc_length, padded_sent_length)\n",
        "        # -> `packed_sents.data` is now of size (num_sents, padded_sent_length)\n",
        "        packed_sents = pack_padded_sequence(docs, lengths=doc_lengths.tolist(), batch_first=True)\n",
        "\n",
        "        # effective batch size at each timestep\n",
        "        docs_valid_bsz = packed_sents.batch_sizes\n",
        "\n",
        "        #print(packed_sents, docs_valid_bsz)\n",
        "\n",
        "\n",
        "        # Make a long batch of sentence lengths by removing pad-sentences\n",
        "        # i.e. `sent_lengths` was of size (num_docs, padded_doc_length)\n",
        "        # -> `packed_sent_lengths.data` is now of size (num_sents)\n",
        "        packed_sent_lengths = pack_padded_sequence(sent_lengths, lengths=doc_lengths.tolist(), batch_first=True)\n",
        "        \n",
        "        #print(packed_sent_lengths)\n",
        "        \n",
        "        # Make a long batch of attention masks by removing pad-sentences\n",
        "        # i.e. `docs` was of size (num_docs, padded_doc_length, padded_sent_length)\n",
        "        # -> `packed_attention_masks.data` is now of size (num_sents, padded_sent_length)\n",
        "        packed_attention_masks = pack_padded_sequence(attention_masks, lengths=doc_lengths.tolist(), batch_first=True)\n",
        "\n",
        "        # Make a long batch of token_type_ids by removing pad-sentences\n",
        "        # i.e. `docs` was of size (num_docs, padded_doc_length, padded_sent_length)\n",
        "        # -> `token_type_ids.data` is now of size (num_sents, padded_sent_length)\n",
        "        packed_token_type_ids = pack_padded_sequence(token_type_ids, lengths=doc_lengths.tolist(), batch_first=True)\n",
        "\n",
        "        sents, sent_lengths, attn_masks, token_types = (\n",
        "            packed_sents.data, packed_sent_lengths.data, packed_attention_masks.data, packed_token_type_ids.data\n",
        "        )\n",
        "\n",
        "        #print(sents, sent_lengths, attn_masks, token_types)\n",
        "        \n",
        "        # Sort sents by decreasing order in sentence lengths\n",
        "        sent_lengths, sent_perm_idx = sent_lengths.sort(dim=0, descending=True)\n",
        "        sents = sents[sent_perm_idx]\n",
        "        \n",
        "        #print(sents.shape, sent_lengths.shape, sent_perm_idx.shape)\n",
        "        return sents, sent_lengths, attn_masks, token_types, docs_valid_bsz, doc_perm_idx, sent_perm_idx"
      ],
      "metadata": {
        "id": "Mh8zAsNOb9RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
        "from transformers import BertModel\n",
        "\n",
        "\n",
        "class WordAttentionPost(torch.nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            device: str,\n",
        "            recurrent_size: int,\n",
        "            attention_dim: int,\n",
        "            bert_version: str\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.attention_dim = attention_dim\n",
        "        self.recurrent_size = recurrent_size\n",
        "        self._device = device\n",
        "        self.bert_model = BertModel.from_pretrained(bert_version)\n",
        "\n",
        "        # Maps BERT output to `attention_dim` sized tensor\n",
        "        self.word_weight = nn.Linear(self.recurrent_size, self.attention_dim)\n",
        "\n",
        "        # Word context vector (u_w) to take dot-product with\n",
        "        self.context_weight = nn.Linear(self.attention_dim, 1)\n",
        "\n",
        "    def recurrent_size(self):\n",
        "        return self.recurrent_size\n",
        "\n",
        "    def forward(self, sents, sent_lengths, attn_masks, token_types, docs_valid_bsz,  doc_perm_idx, sent_perm_idx):\n",
        "        \n",
        "        output_bert = self.bert_model(sents,attention_mask=attn_masks, token_type_ids=token_types)\n",
        "        pooled_out = output_bert.pooler_output\n",
        "        embeddings = output_bert.last_hidden_state\n",
        "        #print(sents.shape)\n",
        "        #print(pooled_out.shape, embeddings.shape)\n",
        "\n",
        "\n",
        "        packed_words = pack_padded_sequence(embeddings, lengths=sent_lengths.tolist(), batch_first=True)\n",
        "\n",
        "        # effective batch size at each timestep\n",
        "        sentences_valid_bsz = packed_words.batch_sizes\n",
        "        #print(sentences_valid_bsz)\n",
        "        #print(packed_words.data.shape)\n",
        "\n",
        "        \n",
        "        u_i = torch.tanh(self.word_weight(packed_words.data))\n",
        "        #print(u_i.shape)\n",
        "        u_w = self.context_weight(u_i).squeeze(1)\n",
        "        #print(u_w.shape)\n",
        "        val = u_w.max()\n",
        "        att = torch.exp(u_w - val)\n",
        "        #print(val, att.shape)\n",
        "        \n",
        "        # Restore as sentences by repadding\n",
        "        att, _ = pad_packed_sequence(PackedSequence(att, sentences_valid_bsz), batch_first=True)\n",
        "\n",
        "        #print(att.shape)\n",
        "\n",
        "        att_weights = att / torch.sum(att, dim=1, keepdim=True)\n",
        "        #print(att,att_weights,att_weights.shape)\n",
        "\n",
        "\n",
        "        # Restore as sentences by repadding\n",
        "        sents, _ = pad_packed_sequence(packed_words, batch_first=True)\n",
        "        #print(sents.shape)\n",
        "\n",
        "        sents = sents * att_weights.unsqueeze(2)\n",
        "        #print(sents.shape)\n",
        "        sents = sents.sum(dim=1)\n",
        "        #print(sents.shape)\n",
        "        \n",
        "        # Restore the original order of sentences (undo the first sorting)\n",
        "        _, sent_unperm_idx = sent_perm_idx.sort(dim=0, descending=False)\n",
        "        sents = sents[sent_unperm_idx]\n",
        "\n",
        "        #print(sents.shape, sent_perm_idx, sent_unperm_idx)\n",
        "        \n",
        "        att_weights = att_weights[sent_unperm_idx]\n",
        "\n",
        "        #print(f'att_wt {att_weights}')\n",
        "        #print(sents.shape)\n",
        "        #print(doc_perm_idx)\n",
        "        #print(docs_valid_bsz)\n",
        "        #return \n",
        "\n",
        "        return sents, doc_perm_idx, docs_valid_bsz, att_weights"
      ],
      "metadata": {
        "id": "JcN8g5XAb_oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import A\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, PackedSequence\n",
        "\n",
        "class SentenceAttention(torch.nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            device: str,\n",
        "            dropout: float,\n",
        "            word_recurrent_size: int,\n",
        "            recurrent_size: int,\n",
        "            attention_dim: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self._device = device\n",
        "        self.word_recurrent_size = word_recurrent_size\n",
        "        self.recurrent_size = recurrent_size\n",
        "        self.dropout = dropout\n",
        "        self.attention_dim = attention_dim\n",
        "\n",
        "        assert self.recurrent_size % 2 == 0\n",
        "\n",
        "        self.encoder = nn.GRU(\n",
        "            input_size=self.word_recurrent_size,\n",
        "            hidden_size=self.recurrent_size // 2,\n",
        "            dropout=self.dropout,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Maps GRU output to `attention_dim` sized tensor\n",
        "        self.sentence_weight = nn.Linear(self.recurrent_size, self.attention_dim)\n",
        "\n",
        "        # Word context vector (u_w) to take dot-product with\n",
        "        self.sentence_context_weight = nn.Linear(self.attention_dim, 1)\n",
        "\n",
        "    def recurrent_size(self):\n",
        "        return self.recurrent_size\n",
        "\n",
        "    def forward(self, sent_embeddings, doc_perm_idx, doc_valid_bsz, word_att_weights):\n",
        "        \"\"\"\n",
        "        :param sent_embeddings: LongTensor (batch_size * padded_doc_length, sentence recurrent dim)\n",
        "        :param doc_perm_idx: LongTensor (batch_size)\n",
        "        :param doc_valid_bsz: LongTensor (max_doc_len)\n",
        "        :param word_att_weights: LongTensor (batch_size * padded_doc_length, max_sent_len)\n",
        "        :return: docs embeddings, word attention weights, sentence attention weights\n",
        "        \"\"\"\n",
        "\n",
        "        sent_embeddings = self.dropout(sent_embeddings)\n",
        "\n",
        "        # Sentence-level LSTM over sentence embeddings\n",
        "        packed_sentences, _ = self.encoder(PackedSequence(sent_embeddings, doc_valid_bsz))\n",
        "        \n",
        "        #print(packed_sentences.data.size())\n",
        "\n",
        "        u_i = torch.tanh(self.sentence_weight(packed_sentences.data))\n",
        "        u_w = self.sentence_context_weight(u_i).squeeze(1)\n",
        "        val = u_w.max()\n",
        "        att = torch.exp(u_w - val)\n",
        "\n",
        "        #print(u_i.size(), att.size(),att)\n",
        "\n",
        "        \n",
        "        # Restore as sentences by repadding\n",
        "        att, _ = pad_packed_sequence(PackedSequence(att, doc_valid_bsz), batch_first=True)\n",
        "\n",
        "        #print(att)\n",
        "        \n",
        "        sent_att_weights = att / torch.sum(att, dim=1, keepdim=True)\n",
        "\n",
        "        #print(sent_att_weights)\n",
        "\n",
        "        # Restore as documents by repadding\n",
        "        docs, _ = pad_packed_sequence(packed_sentences, batch_first=True)\n",
        "\n",
        "        #print(docs.shape)\n",
        "        \n",
        "        # Compute document vectors\n",
        "        docs = docs * sent_att_weights.unsqueeze(2)\n",
        "        docs = docs.sum(dim=1)\n",
        "        #print(docs.shape)\n",
        "\n",
        "        # Restore as documents by repadding\n",
        "        word_att_weights, _ = pad_packed_sequence(PackedSequence(word_att_weights, doc_valid_bsz), batch_first=True)\n",
        "        #print(word_att_weights.shape, word_att_weights)\n",
        "        \n",
        "        # Restore the original order of documents (undo the first sorting)\n",
        "        _, doc_unperm_idx = doc_perm_idx.sort(dim=0, descending=False)\n",
        "        docs = docs[doc_unperm_idx]\n",
        "\n",
        "        #print(docs.shape, doc_perm_idx, doc_unperm_idx)\n",
        "\n",
        "        word_att_weights = word_att_weights[doc_unperm_idx]\n",
        "        sent_att_weights = sent_att_weights[doc_unperm_idx]\n",
        "        #print(word_att_weights, sent_att_weights)\n",
        "        \n",
        "        return docs, word_att_weights, sent_att_weights"
      ],
      "metadata": {
        "id": "TUpSo2racBUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class HANModel(torch.nn.Module):\n",
        "  def __init__(\n",
        "            self,\n",
        "            device: str,\n",
        "            dropout: float,\n",
        "            word_embed_dim: int,\n",
        "            word_att_dim: int,\n",
        "            sentence_embed_dim: int, \n",
        "            sentence_att_dim: int,\n",
        "            bert_version : str\n",
        "            ):\n",
        "        super(HANModel, self).__init__()\n",
        "        self.word_attention_pre = WordAttentionPre(device)\n",
        "        self.word_attention_post = WordAttentionPost(device, word_embed_dim, word_att_dim, bert_version)\n",
        "        #self.word_attention = WordAttention(device, word_embed_dim, word_att_dim, bert_version)\n",
        "        self.sentence_attention = SentenceAttention(device, dropout, word_embed_dim, sentence_embed_dim, sentence_att_dim)\n",
        "        \n",
        "  \n",
        "  def forward(self, docs, doc_lengths, sent_lengths, attention_masks, token_type_ids):\n",
        "        \"\"\"\n",
        "        :param docs: encoded document-level data; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
        "        :param doc_lengths: unpadded document lengths; LongTensor (num_docs)\n",
        "        :param sent_lengths: unpadded sentence lengths; LongTensor (num_docs, max_sent_len)\n",
        "        :param labels: labels; LongTensor (num_docs)\n",
        "        :param attention_masks: BERT attention masks; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
        "        :param token_type_ids: BERT token type IDs; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
        "        :return: class scores, attention weights of words, attention weights of sentences\n",
        "        \"\"\"\n",
        "        \n",
        "        # get sentence embedding for each sentence by passing the input in the world attention model\n",
        "        sents, sent_lengths, attn_masks, token_types, docs_valid_bsz,  doc_perm_idx, sent_perm_idx = self.word_attention_pre(\n",
        "                docs, doc_lengths, sent_lengths, attention_masks, token_type_ids\n",
        "            )\n",
        "        \n",
        "        sent_embeddings, doc_perm_idx, docs_valid_bsz, word_att_weights = self.word_attention_post(\n",
        "                 sents, sent_lengths, attn_masks, token_types, docs_valid_bsz,  doc_perm_idx, sent_perm_idx\n",
        "            )\n",
        "\n",
        "        '''\n",
        "        sent_embeddings, doc_perm_idx, docs_valid_bsz, word_att_weights = self.word_attention(\n",
        "                docs, doc_lengths, sent_lengths, attention_masks, token_type_ids\n",
        "            )\n",
        "        '''\n",
        "        # get document embedding for each document by passing the sentence embeddings in the sentence attention model\n",
        "        doc_embeds, word_att_weights, sentence_att_weights = self.sentence_attention(\n",
        "            sent_embeddings, doc_perm_idx, docs_valid_bsz, word_att_weights\n",
        "        )\n",
        "        \n",
        "\n",
        "        return (doc_embeds,word_att_weights, sentence_att_weights)"
      ],
      "metadata": {
        "id": "FO3IJyLbcDQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class HANClassifierModel(torch.nn.Module):\n",
        "  def __init__(\n",
        "            self,\n",
        "            device: str,\n",
        "            dropout: float,\n",
        "            word_embed_dim: int,\n",
        "            word_att_dim: int,\n",
        "            sentence_embed_dim: int, \n",
        "            sentence_att_dim: int,\n",
        "            classifier_hidden_dim: int, \n",
        "            num_classes :int,\n",
        "            bert_version : str,\n",
        "            task = None : str\n",
        "            ):\n",
        "        super(HANClassifierModel, self).__init__()\n",
        "\n",
        "        self.HANmodel = HANModel(device, dropout, word_embed_dim, word_att_dim,sentence_embed_dim,sentence_att_dim,bert_version) \n",
        "\n",
        "        if task == 'ab':\n",
        "          sentence_embed_dim += num_classes\n",
        "          \n",
        "        self.classifier = nn.Sequential(\n",
        "            torch.nn.Linear(sentence_embed_dim, classifier_hidden_dim), \n",
        "            nn.ReLU(), \n",
        "            nn.Dropout(dropout),\n",
        "            torch.nn.Linear(classifier_hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "  \n",
        "  \n",
        "  def forward(self, docs, doc_lengths, sent_lengths, attention_masks, token_type_ids):\n",
        "        \"\"\"\n",
        "        :param docs: encoded document-level data; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
        "        :param doc_lengths: unpadded document lengths; LongTensor (num_docs)\n",
        "        :param sent_lengths: unpadded sentence lengths; LongTensor (num_docs, max_sent_len)\n",
        "        :param labels: labels; LongTensor (num_docs)\n",
        "        :param attention_masks: BERT attention masks; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
        "        :param token_type_ids: BERT token type IDs; LongTensor (num_docs, padded_doc_length, padded_sent_length)\n",
        "        :return: class scores, attention weights of words, attention weights of sentences\n",
        "        \"\"\"\n",
        "        \n",
        "        # get document embedding for each document by passing the sentence embeddings in the sentence attention model\n",
        "        doc_embeds, word_att_weights, sentence_att_weights = self.HANmodel(docs, doc_lengths, sent_lengths, attention_masks, token_type_ids)\n",
        "\n",
        "        if task == 'ab':\n",
        "          doc_embeds = torch.cat((doc_embeds, b_labels), 1)\n",
        "        \n",
        "        scores = self.classifier(doc_embeds)\n",
        "\n",
        "        outputs = (scores, word_att_weights, sentence_att_weights)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "Sh9YLQgEcFsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "\n",
        "class GradientReversalFunction(Function):\n",
        "    \"\"\"\n",
        "    Forward pass is the identity function. In the backward pass,\n",
        "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambda_):\n",
        "        ctx.lambda_ = lambda_\n",
        "        return x.clone()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grads):\n",
        "        lambda_ = ctx.lambda_\n",
        "        lambda_ = grads.new_tensor(lambda_)\n",
        "        dx = -lambda_ * grads\n",
        "        return dx, None\n",
        "\n",
        "\n",
        "class GradientReversal(torch.nn.Module):\n",
        "    def __init__(self, lambda_=1):\n",
        "        super(GradientReversal, self).__init__()\n",
        "        self.lambda_ = lambda_\n",
        "\n",
        "    def forward(self, x):\n",
        "        return GradientReversalFunction.apply(x, self.lambda_)"
      ],
      "metadata": {
        "id": "ATAw4bsrev-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task J Data Loading"
      ],
      "metadata": {
        "id": "P7yOcqqYcJW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torchtext\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, KBinsDiscretizer\n",
        "\n",
        "def len_condition(each_text):\n",
        "  k = sum([1 if re.match(r'^\\d+\\. ',x) is not None else 0 for x in each_text])\n",
        "  return k if k!=0 else len(each_text)\n",
        "\n",
        "\n",
        "class ECHRDataset(Dataset):\n",
        "  def __init__(self, path, tokenizer_path, max_sent_len, max_doc_len,classes=None,scaler=None,paragraph_num_removal = False, vocab_confounder = None):\n",
        "    self.max_sent_len = max_sent_len\n",
        "    self.max_doc_len = max_doc_len\n",
        "    self.encoding = []\n",
        "    self.labels = []\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    self.sep_token_id = self.tokenizer.sep_token_id\n",
        "    self.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "    df = pd.read_pickle(path)\n",
        "    df = df.loc[df.RESPONDENT.str.len()==1]\n",
        "    df = df.reset_index()\n",
        "\n",
        "    self.labels = df[\"label\"].to_numpy()\n",
        "\n",
        "    df['len'] = df[\"TEXT\"].apply(lambda row: len_condition(row))\n",
        "\n",
        "    if paragraph_num_removal :\n",
        "      df[\"TEXT\"] = df[\"TEXT\"].apply(lambda row: [re.sub(r'^\\d+\\. ', '',sentence)  for sentence in row])   \n",
        "      \n",
        "\n",
        "    text_list = df[\"TEXT\"].apply(lambda row: [self.tokenizer.encode(sentence) for sentence in row])\n",
        "    spl_tokens = [self.tokenizer.sep_token_id, self.tokenizer.cls_token_id]\n",
        "    text_list = text_list.apply(lambda row:  [self.filter(sentence, spl_tokens) for sentence in row])\n",
        "    self.encoding = text_list.apply(lambda row: self.pack_sentences(row))\n",
        "\n",
        "    if(vocab_confounder):\n",
        "       cf_tok = list(vocab_confounder)\n",
        "       self.vocab_cf = df[\"TEXT\"].apply(lambda x: 1 if len(set((\" \".join(x)).split()).intersection(set(cf_tok)))!=0 else 0)\n",
        "    \n",
        "    mlb = MultiLabelBinarizer()\n",
        "    if classes is None:\n",
        "      self.respondents = mlb.fit_transform(df.pop('RESPONDENT'))\n",
        "    else:\n",
        "      mlb.fit([classes])\n",
        "      self.respondents = mlb.transform(df.pop('RESPONDENT'))\n",
        "      \n",
        "    \n",
        "    if scaler is None:\n",
        "      self.scaler = KBinsDiscretizer(n_bins=5, encode='onehot-dense', strategy='kmeans')\n",
        "      self.length = self.scaler.fit_transform(df[['len']])\n",
        "    else:\n",
        "      self.length = scaler.transform(df[['len']])\n",
        "\n",
        "    self.classes = mlb.classes_\n",
        "    # Number of examples.\n",
        "    self.n_examples = len(self.labels)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_examples\n",
        "\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return (self.encoding[item], self.labels[item], self.respondents[item], list(map(int,self.length[item])), self.vocab_cf[item])\n",
        "  \n",
        "  def filter(self, sentence, spl_tokens):\n",
        "    return [y for y in sentence if y  not in spl_tokens]\n",
        "\n",
        "  \n",
        "  def pack_sentences(self,token_encodings):\n",
        "    doc = []\n",
        "    sentence_len = [len(x) for x in token_encodings]\n",
        "    i = 0\n",
        "    sum = 0\n",
        "    each_sentence_group = []\n",
        "    \n",
        "    while i <len(sentence_len):\n",
        "      while(sentence_len[i]> self.max_sent_len):\n",
        "        if(len(each_sentence_group)!=0):\n",
        "            doc.append(each_sentence_group)\n",
        "        doc.append(token_encodings[i][:self.max_sent_len])\n",
        "        sentence_len[i] = sentence_len[i] - self.max_sent_len\n",
        "        token_encodings[i] = token_encodings[i][self.max_sent_len:]\n",
        "        each_sentence_group = []\n",
        "        sum = 0\n",
        "      sum += sentence_len[i] \n",
        "      if(sum<= self.max_sent_len):\n",
        "        each_sentence_group.extend(token_encodings[i])\n",
        "      else:\n",
        "        doc.append(each_sentence_group)\n",
        "        each_sentence_group = token_encodings[i]\n",
        "        sum = sentence_len[i] \n",
        "      i +=1\n",
        "    \n",
        "    if(len(each_sentence_group)!=0):\n",
        "      doc.append(each_sentence_group)\n",
        "    \n",
        "    if(len(doc)>self.max_doc_len):\n",
        "      doc = doc[:self.max_doc_len]\n",
        "    return doc"
      ],
      "metadata": {
        "id": "Dyn3SVWWcMHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCollate:\n",
        "    def __init__(self, pad_idx, sep_idx):\n",
        "        self.pad_token_id = pad_idx\n",
        "        self.sep_token_id = sep_idx\n",
        "\n",
        "    def pad_sentence_for_batch(self, tokens_lists, max_len: int):\n",
        "        pad_id = self.pad_token_id\n",
        "        toks_ids = []\n",
        "        att_masks = []\n",
        "        tok_type_lists = []\n",
        "        for item_toks in tokens_lists:\n",
        "            padded_item_toks = item_toks + [pad_id] * (max_len - len(item_toks))\n",
        "            toks_ids.append(padded_item_toks)\n",
        "\n",
        "            att_mask = [1] * len(item_toks) + [0] * (max_len - len(item_toks))\n",
        "            att_masks.append(att_mask)\n",
        "\n",
        "            tok_type_list = [0] * (max_len)\n",
        "            tok_type_lists.append(tok_type_list)\n",
        "            \n",
        "        return toks_ids, att_masks, tok_type_lists\n",
        "\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        batch = filter(lambda x: x is not None, batch)\n",
        "        docs, labels, country_labels, length, vocab_cf = list(zip(*batch))\n",
        "        doc_lengths = [len(x) for x in docs]\n",
        "        sent_lengths = []\n",
        "        for element in docs:\n",
        "          sent_lengths.append([len(i) for i in element])\n",
        "        \n",
        "        batch_sz = len(labels)\n",
        "        batch_max_doc_length = max(doc_lengths)\n",
        "        batch_max_sent_length = max([max(sl) for sl in sent_lengths])\n",
        "\n",
        "        docs_tensor = torch.zeros((batch_sz, batch_max_doc_length, batch_max_sent_length), dtype=torch.long)\n",
        "        batch_att_mask_tensor = torch.zeros((batch_sz, batch_max_doc_length, batch_max_sent_length), dtype=torch.long)\n",
        "        token_type_ids_tensor = torch.zeros((batch_sz, batch_max_doc_length, batch_max_sent_length), dtype=torch.long)\n",
        "        sent_lengths_tensor = torch.zeros((batch_sz, batch_max_doc_length))\n",
        "\n",
        "        for doc_idx, doc in enumerate(docs):\n",
        "            padded_token_lists, att_mask_lists, tok_type_lists = self.pad_sentence_for_batch(doc, batch_max_sent_length)\n",
        "\n",
        "            doc_length = doc_lengths[doc_idx]\n",
        "            sent_lengths_tensor[doc_idx, :doc_length] = torch.tensor(sent_lengths[doc_idx], dtype=torch.long)\n",
        "\n",
        "            for sent_idx, (padded_tokens, att_masks, tok_types) in enumerate(\n",
        "                    zip(padded_token_lists, att_mask_lists, tok_type_lists)):\n",
        "                docs_tensor[doc_idx, sent_idx, :] = torch.tensor(padded_tokens, dtype=torch.long)\n",
        "                batch_att_mask_tensor[doc_idx, sent_idx, :] = torch.tensor(att_masks, dtype=torch.long)\n",
        "                token_type_ids_tensor[doc_idx, sent_idx, :] = torch.tensor(tok_types, dtype=torch.long)\n",
        "\n",
        "        return (\n",
        "            docs_tensor,\n",
        "            torch.tensor(labels, dtype=torch.long),\n",
        "            torch.tensor(country_labels, dtype=torch.long),\n",
        "            torch.tensor(length,dtype=torch.long),\n",
        "            torch.tensor(vocab_cf, dtype=torch.long),\n",
        "            torch.tensor(doc_lengths, dtype=torch.long),\n",
        "            sent_lengths_tensor,\n",
        "            batch_att_mask_tensor, \n",
        "            token_type_ids_tensor,\n",
        "        )"
      ],
      "metadata": {
        "id": "k4dXRkKocNm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph_num_removal = True\n",
        "vocab_confounder = ['represented', 'national', 'mr', 'summarised', 'practising', 'lawyer', 'agent', 'paragraph']\n",
        "\n",
        "train_dataset = ECHRDataset('train.pkl',tokenizer_path = \"nlpaueb/legal-bert-base-uncased\", max_sent_len = 500, max_doc_len = 40,paragraph_num_removal = paragraph_num_removal, vocab_confounder = vocab_confounder,vocab_depth =vocab_depth )\n",
        "dev_dataset = ECHRDataset('dev.pkl',tokenizer_path = \"nlpaueb/legal-bert-base-uncased\", max_sent_len = 500, max_doc_len = 40, classes= train_dataset.classes,scaler=train_dataset.scaler,paragraph_num_removal = paragraph_num_removal,vocab_confounder = vocab_confounder)\n",
        "test_dataset = ECHRDataset('test.pkl',tokenizer_path = \"nlpaueb/legal-bert-base-uncased\", max_sent_len = 500, max_doc_len = 40, classes= train_dataset.classes,scaler=train_dataset.scaler,paragraph_num_removal = paragraph_num_removal,vocab_confounder = vocab_confounder)"
      ],
      "metadata": {
        "id": "ZfcD17E6cSI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader =  DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn = MyCollate(pad_idx = train_dataset.pad_token_id, sep_idx = train_dataset.pad_token_id))\n",
        "dev_dataloader =  DataLoader(dev_dataset, batch_size=16, shuffle=True, collate_fn = MyCollate(pad_idx = dev_dataset.pad_token_id, sep_idx = dev_dataset.pad_token_id))\n",
        "test_dataloader =  DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn = MyCollate(pad_idx = test_dataset.pad_token_id, sep_idx = test_dataset.pad_token_id))"
      ],
      "metadata": {
        "id": "HUjCkthdfAOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training only violation prediction Model"
      ],
      "metadata": {
        "id": "daG25n-jezPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
        "import itertools\n",
        "\n",
        "\n",
        "num_epochs = 20\n",
        "learning_rate = 5e-4\n",
        "file_prefix = \"model_par_rem_\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = HANClassifierModel(device, 0.1, 768, 300, 400, 200, 100, 1,\"nlpaueb/legal-bert-base-uncased\").to(device)\n",
        "feature_extractor = model.HANmodel\n",
        "classifier = model.classifier\n",
        "\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "  if(\"bert\" in name):\n",
        "    param.requires_grad = False\n",
        "\n",
        "optimizer = optim.Adam(list(filter(lambda p: p.requires_grad, model.parameters())), lr=learning_rate)\n",
        "cls_criterion= nn.BCEWithLogitsLoss()\n",
        "\n",
        "column_names = [\"epoch\", \"mean epoch loss\", \"accuracy\",\"precision\",\"recall\",\"f1\"]\n",
        "train_df = pd.DataFrame(columns = column_names)\n",
        "dev_df = pd.DataFrame(columns = column_names)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "    model.train()\n",
        "    cls_predicted, cls_gold,loss_batch = [],[],[]\n",
        "    for batch_idx, (docs,labels, country_labels, length, vocab_cf, doc_lengths, sent_lengths, att_mask, token_type_id) in tqdm(enumerate(train_dataloader),total=len(train_dataloader), leave=False):\n",
        "        # Get input and targets and get to cuda\n",
        "        docs, labels, country_labels, length,vocab_cf, doc_lengths, sent_lengths, att_mask, token_type_id = docs.to(device), labels.to(device),country_labels.to(device), length.to(device),vocab_cf.to(length), doc_lengths.to(device), sent_lengths.to(device), att_mask.to(device), token_type_id.to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        doc_embeds, word_att_weights, sentence_att_weights = feature_extractor(docs, doc_lengths, sent_lengths, att_mask, token_type_id)\n",
        "\n",
        "        cls_scores = classifier(doc_embeds)\n",
        "        cls_scores = cls_scores.squeeze(dim=1)        \n",
        "        \n",
        "        cls_loss = cls_criterion(cls_scores, labels.float())\n",
        "        \n",
        "        loss = cls_loss\n",
        "        loss_batch.append(loss)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        cls_preds = torch.sigmoid(cls_scores) # Sigmoid to map predictions between 0 and 1\n",
        "        cls_pred_labels = (cls_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "        cls_gold.extend(labels.cpu().detach().numpy())\n",
        "        cls_predicted.extend(cls_pred_labels.cpu().detach().numpy())\n",
        "\n",
        "    mean_epoch_loss = (torch.stack(loss_batch , dim=0).sum(dim=0)).item()\n",
        "\n",
        "    cls_accuracy, cls_precision, cls_recall, cls_f1 = accuracy_score(cls_gold,cls_predicted),precision_score(cls_gold, cls_predicted),recall_score(cls_gold, cls_predicted),f1_score(cls_gold, cls_predicted)\n",
        "    #print(f\"Train Cls Statistics : Accuracy : {100.0*cls_accuracy:4.2f}%, {100.0*cls_precision:4.2f}%,{100.0*cls_recall:4.2f}%,{100.0*cls_f1:4.2f}%\")   \n",
        "    train_df.loc[len(train_df)] = [epoch,round(mean_epoch_loss,2),round(100*cls_accuracy,2), round(100*cls_precision,2), round(100*cls_recall,2), round(100*cls_f1,2)]\n",
        "\n",
        "    model_checkpoint = {'model_state_dict':model.state_dict(),'optimizer': optimizer.state_dict()}\n",
        "    torch.save(model_checkpoint,file_prefix+str(epoch)+\".pth.tar\")\n",
        "\n",
        "    model.eval() \n",
        "    cls_predicted, cls_gold,loss_batch = [],[],[]\n",
        "\n",
        "    with torch.no_grad(): \n",
        "      for batch_idx, (docs,labels, country_labels, length, vocab_cf,doc_lengths, sent_lengths, att_mask, token_type_id) in tqdm(enumerate(dev_dataloader),total=len(dev_dataloader), leave=False):\n",
        "        # Get input and targets and get to cuda\n",
        "        docs, labels, country_labels, length,vocab_cf, doc_lengths, sent_lengths, att_mask, token_type_id = docs.to(device), labels.to(device),country_labels.to(device),length.to(device),vocab_cf.to(device), doc_lengths.to(device), sent_lengths.to(device), att_mask.to(device), token_type_id.to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        doc_embeds, word_att_weights, sentence_att_weights = feature_extractor(docs, doc_lengths, sent_lengths, att_mask, token_type_id)\n",
        "\n",
        "        #disc_scores = country_discriminator(doc_embeds)\n",
        "        cls_scores = classifier(doc_embeds)\n",
        "\n",
        "        cls_scores = cls_scores.squeeze(dim=1)\n",
        "        cls_loss = cls_criterion(cls_scores, labels.float())\n",
        "\n",
        "        loss = cls_loss\n",
        "        loss_batch.append(loss)\n",
        "        \n",
        "        cls_preds = torch.sigmoid(cls_scores) # Sigmoid to map predictions between 0 and 1\n",
        "        cls_pred_labels = (cls_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "        cls_gold.extend(labels.cpu().detach().numpy())\n",
        "        cls_predicted.extend(cls_pred_labels.cpu().detach().numpy())\n",
        "\n",
        "\n",
        "    mean_epoch_loss = (torch.stack(loss_batch , dim=0).sum(dim=0)).item()\n",
        "\n",
        "    cls_accuracy, cls_precision, cls_recall, cls_f1 = accuracy_score(cls_gold,cls_predicted),precision_score(cls_gold, cls_predicted),recall_score(cls_gold, cls_predicted),f1_score(cls_gold, cls_predicted)\n",
        "    #print(f\"Dev Cls Statistics : Accuracy : {100.0*cls_accuracy:4.2f}%, {100.0*cls_precision:4.2f}%,{100.0*cls_recall:4.2f}%,{100.0*cls_f1:4.2f}%\")   \n",
        "    dev_df.loc[len(dev_df)] = [epoch,round(mean_epoch_loss,2),round(100*cls_accuracy,2), round(100*cls_precision,2), round(100*cls_recall,2), round(100*cls_f1,2)]\n",
        "\n",
        "    print(train_df)\n",
        "    print(dev_df)    "
      ],
      "metadata": {
        "id": "vtnHB75Ee36H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deconfounding Country/len/vocab/all "
      ],
      "metadata": {
        "id": "PQM_aWpFfR-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
        "import itertools\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "violation_learning_rate = 5e-5\n",
        "country_learning_rate = 5e-4\n",
        "length_learning_rate = 5e-4\n",
        "vocab_learning_rate = 5e-4\n",
        "\n",
        "gamma_country = 0.1\n",
        "gamma_length = 0.1\n",
        "gamma_vocab = 0.1\n",
        "country_only = False\n",
        "length_only = False\n",
        "vocab_only = False\n",
        "country_and_length_and_vocab = True\n",
        "file_prefix = 'grad_vocab_all_'\n",
        "\n",
        "assert country_only ^ length_only ^ vocab_only ^ country_and_length_and_vocab\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = HANClassifierModel(device, 0.1, 768, 300, 400, 200, 100, 1,\"nlpaueb/legal-bert-base-uncased\").to(device)\n",
        "\n",
        "\n",
        "feature_extractor = model.HANmodel\n",
        "classifier = model.classifier\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "  if(\"bert\" in name):\n",
        "    param.requires_grad = False\n",
        "\n",
        "params_dict = []\n",
        "params_dict.append({'params': list(filter(lambda p: p.requires_grad, model.parameters())), 'lr': violation_learning_rate})\n",
        "\n",
        "country_modules  = []\n",
        "len_modules = []\n",
        "vocab_modules = []\n",
        "\n",
        "\n",
        "if  country_only or country_and_length_and_vocab:\n",
        "  country_modules = country_modules + [\n",
        "              torch.nn.Linear(400, 100), \n",
        "              nn.ReLU(), \n",
        "              nn.Dropout(0.1),\n",
        "              torch.nn.Linear(100,len(train_dataset.classes))\n",
        "            ]\n",
        "  country_discriminator = nn.Sequential(*country_modules).to(device)\n",
        "  params_dict.append({'params': list(country_discriminator.parameters()), 'lr': country_learning_rate})\n",
        "\n",
        "if length_only or country_and_length_and_vocab:\n",
        "  len_modules =  len_modules + [\n",
        "              torch.nn.Linear(400, 100), \n",
        "              nn.ReLU(), \n",
        "              nn.Dropout(0.1),\n",
        "              torch.nn.Linear(100,5)\n",
        "            ]\n",
        "  len_discriminator = nn.Sequential(*len_modules).to(device)\n",
        "  params_dict.append({'params': list(len_discriminator.parameters()), 'lr': length_learning_rate})\n",
        "\n",
        "if vocab_only or country_and_length_and_vocab:\n",
        "  vocab_modules =  vocab_modules + [\n",
        "              torch.nn.Linear(400, 100), \n",
        "              nn.ReLU(), \n",
        "              nn.Dropout(0.1),\n",
        "              torch.nn.Linear(100,8)\n",
        "            ]\n",
        "  vocab_discriminator = nn.Sequential(*vocab_modules).to(device)\n",
        "  params_dict.append({'params': list(vocab_discriminator.parameters()), 'lr': vocab_learning_rate})\n",
        "\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(params_dict)\n",
        "\n",
        "\n",
        "cls_criterion= nn.BCEWithLogitsLoss()\n",
        "disc_criterion = nn.CrossEntropyLoss()\n",
        "len_criterion = nn.CrossEntropyLoss()\n",
        "vocab_criterion= nn.BCEWithLogitsLoss()\n",
        "\n",
        "column_names = [\"cls_loss\", \"cls_accuracy\",\"cls_precision\",\"cls_recall\",\"cls_f1\",\"disc_loss\", \"disc_accuracy\",\"disc_precision\",\"disc_recall\",\"disc_f1\",\"len_loss\", \"len_accuracy\",\"len_precision\",\"len_recall\",\"len_f1\",\"vocab_loss\", \"vocab_accuracy\",\"vocab_precision\",\"vocab_recall\",\"vocab_f1\"]\n",
        "train_df = pd.DataFrame(columns = column_names)\n",
        "dev_df = pd.DataFrame(columns = column_names)\n",
        "\n",
        "def get_lambda(gamma, p):\n",
        "   return 2. / (1. + np.exp(-gamma * p)) - 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "    model.train()\n",
        "    if country_only or country_and_length_and_vocab:\n",
        "        country_discriminator.train() \n",
        "    if length_only or country_and_length_and_vocab: \n",
        "        len_discriminator.train() \n",
        "    if vocab_only or country_and_length_and_vocab: \n",
        "        vocab_discriminator.train() \n",
        "\n",
        "    cls_predicted, cls_gold,cls_loss_batch = [],[],[]\n",
        "    disc_predicted, disc_gold, disc_loss_batch = [],[],[]\n",
        "    vocab_predicted, vocab_gold,vocab_loss_batch = [],[],[]\n",
        "    len_predicted, len_gold, len_loss_batch = [],[],[]\n",
        "\n",
        "    current_iteration = 0\n",
        "    for batch_idx, (docs,labels, country_labels,length,vocab, doc_lengths, sent_lengths, att_mask, token_type_id) in tqdm(enumerate(train_dataloader),total=len(train_dataloader), leave=False):\n",
        "        \n",
        "        current_iteration += 1\n",
        "        p = float(current_iteration + epoch * len(src_train_dataloader)) / num_epochs / len(src_train_dataloader)\n",
        "        lambda_country, lambda_length, lambda_vocab = get_lambda(gamma_country,p), get_lambda(gamma_length,p),get_lambda(gamma_vocab,p)\n",
        "        \n",
        "        # Get input and targets and get to cuda\n",
        "        docs, labels, country_labels,length,vocab, doc_lengths, sent_lengths, att_mask, token_type_id = docs.to(device), labels.to(device),country_labels.to(device),length.to(device),vocab.to(device), doc_lengths.to(device), sent_lengths.to(device), att_mask.to(device), token_type_id.to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        doc_embeds, word_att_weights, sentence_att_weights = feature_extractor(docs, doc_lengths, sent_lengths, att_mask, token_type_id)\n",
        "\n",
        "        cls_scores = classifier(doc_embeds)\n",
        "        cls_scores = cls_scores.squeeze(dim=1)\n",
        "        cls_loss = cls_criterion(cls_scores, labels.float()) \n",
        "        cls_loss_batch.append(cls_loss)       \n",
        "        loss = cls_loss \n",
        "        \n",
        "        \n",
        "        if country_only or country_and_length_and_vocab:  \n",
        "            doc_embeds_country = GradientReversalFunction.apply(doc_embeds, lambda_country)\n",
        "            disc_scores = country_discriminator(doc_embeds_country)\n",
        "            disc_loss = disc_criterion(disc_scores, country_labels.argmax(dim=1))  \n",
        "            disc_loss_batch.append(disc_loss)\n",
        "            loss = loss + disc_loss\n",
        "        if length_only or country_and_length_and_vocab: \n",
        "            doc_embeds_length = GradientReversalFunction.apply(doc_embeds, lambda_length)   \n",
        "            len_scores = len_discriminator(doc_embeds_length)\n",
        "            len_loss = len_criterion(len_scores, length.argmax(dim=1)) \n",
        "            len_loss_batch.append(len_loss)\n",
        "            loss = loss + len_loss  \n",
        "        if vocab_only or country_and_length_and_vocab:  \n",
        "            doc_embeds_vocab = GradientReversalFunction.apply(doc_embeds, lambda_vocab)\n",
        "            vocab_scores = vocab_discriminator(doc_embeds_vocab)\n",
        "            vocab_scores = vocab_scores.flatten()\n",
        "            vocab_loss = vocab_criterion(vocab_scores, vocab.float().flatten()) \n",
        "            vocab_loss_batch.append(vocab_loss)  \n",
        "            loss = loss + vocab_loss  \n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cls_preds = torch.sigmoid(cls_scores) # Sigmoid to map predictions between 0 and 1\n",
        "        cls_pred_labels = (cls_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "        cls_gold.extend(labels.cpu().detach().numpy())\n",
        "        cls_predicted.extend(cls_pred_labels.cpu().detach().numpy())\n",
        "\n",
        "        if country_only or country_and_length_and_vocab: \n",
        "            disc_preds = torch.softmax(disc_scores, dim=1) \n",
        "            disc_gold.extend(country_labels.argmax(dim=1).cpu().detach().numpy())\n",
        "            disc_predicted.extend(disc_preds.argmax(dim=1).cpu().detach().numpy())\n",
        "        \n",
        "        if length_only or country_and_length_and_vocab: \n",
        "            len_preds = torch.softmax(len_scores, dim=1) \n",
        "            len_gold.extend(length.argmax(dim=1).cpu().detach().numpy())\n",
        "            len_predicted.extend(len_preds.argmax(dim=1).cpu().detach().numpy())\n",
        "\n",
        "        if vocab_only or country_and_length_and_vocab: \n",
        "            vocab_preds = torch.sigmoid(vocab_scores) # Sigmoid to map predictions between 0 and 1\n",
        "            vocab_pred_labels = (vocab_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "            #vocab_gold.extend(vocab.cpu().detach().numpy())\n",
        "            vocab_gold.extend(vocab.flatten().cpu().detach().numpy())\n",
        "            vocab_predicted.extend(vocab_pred_labels.cpu().detach().numpy())\n",
        "\n",
        "    cls_epoch_loss = (torch.stack(cls_loss_batch , dim=0).sum(dim=0)).item()\n",
        "    if country_only or country_and_length_and_vocab: \n",
        "        disc_epoch_loss = (torch.stack(disc_loss_batch , dim=0).sum(dim=0)).item()\n",
        "    if length_only or country_and_length_and_vocab:   \n",
        "        len_epoch_loss = (torch.stack(len_loss_batch , dim=0).sum(dim=0)).item()\n",
        "    if vocab_only or country_and_length_and_vocab:   \n",
        "        vocab_epoch_loss = (torch.stack(vocab_loss_batch , dim=0).sum(dim=0)).item()\n",
        "\n",
        "    cls_accuracy, cls_precision, cls_recall, cls_f1 = accuracy_score(cls_gold,cls_predicted),precision_score(cls_gold, cls_predicted),recall_score(cls_gold, cls_predicted),f1_score(cls_gold, cls_predicted)\n",
        "    #print(f\"Train Cls Statistics : Accuracy : {100.0*cls_accuracy:4.2f}%, {100.0*cls_precision:4.2f}%,{100.0*cls_recall:4.2f}%,{100.0*cls_f1:4.2f}%\")   \n",
        "    \n",
        "    if country_only or country_and_length_and_vocab: \n",
        "        disc_accuracy, disc_precision, disc_recall, disc_f1 = accuracy_score(disc_gold,disc_predicted),precision_score(disc_gold, disc_predicted,average='macro'),recall_score(disc_gold, disc_predicted,average='macro'),f1_score(disc_gold,disc_predicted,average='macro')\n",
        "        #print(f\"Train Disc Statistics : Accuracy : {100.0*disc_accuracy:4.2f}%, {100.0*disc_precision:4.2f}%,{100.0*disc_recall:4.2f}%,{100.0*disc_f1:4.2f}%\")   \n",
        "    \n",
        "    if vocab_only or country_and_length_and_vocab: \n",
        "        vocab_accuracy, vocab_precision, vocab_recall, vocab_f1 = accuracy_score(vocab_gold,vocab_predicted),precision_score(vocab_gold, vocab_predicted),recall_score(vocab_gold, vocab_predicted),f1_score(vocab_gold, vocab_predicted)\n",
        "\n",
        "    if length_only or country_and_length_and_vocab: \n",
        "        len_accuracy, len_precision, len_recall, len_f1 = accuracy_score(len_gold,len_predicted),precision_score(len_gold, len_predicted,average='macro'),recall_score(len_gold, len_predicted,average='macro'),f1_score(len_gold, len_predicted,average='macro')\n",
        "\n",
        "    if country_only:\n",
        "        train_df.loc[len(train_df)] = [round(cls_epoch_loss,2),round(100*cls_accuracy,2), round(100*cls_precision,2), round(100*cls_recall,2), round(100*cls_f1,2),round(disc_epoch_loss,2),round(100*disc_accuracy,2), round(100*disc_precision,2), round(100*disc_recall,2), round(100*disc_f1,2),\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]\n",
        "        model_checkpoint = {'model_state_dict':model.state_dict(),'disc_state_dict': country_discriminator.state_dict(),'optimizer': optimizer.state_dict()}\n",
        "    \n",
        "    if length_only:\n",
        "        train_df.loc[len(train_df)] = [round(cls_epoch_loss,2),round(100*cls_accuracy,2), round(100*cls_precision,2), round(100*cls_recall,2), round(100*cls_f1,2),\"\",\"\",\"\",\"\",\"\",round(len_epoch_loss,2),round(100*len_accuracy,2), round(100*len_precision,2), round(100*len_recall,2), round(100*len_f1,2),\"\",\"\",\"\",\"\",\"\"]\n",
        "        model_checkpoint = {'model_state_dict':model.state_dict(),'len_state_dict': len_discriminator.state_dict(),'optimizer': optimizer.state_dict()}\n",
        "    \n",
        "    if vocab_only:\n",
        "        train_df.loc[len(train_df)] = [round(cls_epoch_loss,2),round(100*cls_accuracy,2), round(100*cls_precision,2), round(100*cls_recall,2), round(100*cls_f1,2),\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",round(vocab_epoch_loss,2),round(100*vocab_accuracy,2), round(100*vocab_precision,2), round(100*vocab_recall,2), round(100*vocab_f1,2)]\n",
        "        model_checkpoint = {'model_state_dict':model.state_dict(),'vocab_state_dict': vocab_discriminator.state_dict(),'optimizer': optimizer.state_dict()}\n",
        "    \n",
        "\n",
        "    if country_and_length_and_vocab:\n",
        "        train_df.loc[len(train_df)] = [round(cls_epoch_loss,2),round(100*cls_accuracy,2), round(100*cls_precision,2), round(100*cls_recall,2), round(100*cls_f1,2),round(disc_epoch_loss,2),round(100*disc_accuracy,2), round(100*disc_precision,2), round(100*disc_recall,2), round(100*disc_f1,2),round(len_epoch_loss,2),round(100*len_accuracy,2), round(100*len_precision,2), round(100*len_recall,2), round(100*len_f1,2),round(vocab_epoch_loss,2),round(100*vocab_accuracy,2), round(100*vocab_precision,2), round(100*vocab_recall,2), round(100*vocab_f1,2)]\n",
        "        model_checkpoint = {'model_state_dict':model.state_dict(),'disc_state_dict': country_discriminator.state_dict(),'len_state_dict': len_discriminator.state_dict(),'vocab_state_dict': vocab_discriminator.state_dict(),'optimizer': optimizer.state_dict()}\n",
        "    \n",
        "    \n",
        "    torch.save(model_checkpoint,file_prefix+str(epoch)+\".pth.tar\")\n",
        "\n",
        "    model.eval()\n",
        "    if country_only or country_and_length_and_vocab:\n",
        "        country_discriminator.eval() \n",
        "    if length_only or country_and_length_and_vocab: \n",
        "        len_discriminator.eval() \n",
        "    if vocab_only or country_and_length_and_vocab: \n",
        "        vocab_discriminator.eval() \n",
        "    cls_predicted, cls_gold,cls_loss_batch = [],[],[]\n",
        "    disc_predicted, disc_gold,disc_loss_batch = [],[],[]\n",
        "    vocab_predicted, vocab_gold, vocab_loss_batch = [],[],[]\n",
        "    len_predicted, len_gold, len_loss_batch = [],[],[]\n",
        "    with torch.no_grad(): \n",
        "      for batch_idx, (docs,labels, country_labels,length, vocab, doc_lengths, sent_lengths, att_mask, token_type_id) in tqdm(enumerate(dev_dataloader),total=len(dev_dataloader), leave=False):\n",
        "        # Get input and targets and get to cuda\n",
        "        docs, labels, country_labels,length,vocab, doc_lengths, sent_lengths, att_mask, token_type_id = docs.to(device), labels.to(device),country_labels.to(device),length.to(device),vocab.to(device), doc_lengths.to(device), sent_lengths.to(device), att_mask.to(device), token_type_id.to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        doc_embeds, word_att_weights, sentence_att_weights = feature_extractor(docs, doc_lengths, sent_lengths, att_mask, token_type_id)\n",
        "\n",
        "        cls_scores = classifier(doc_embeds)\n",
        "        cls_scores = cls_scores.squeeze(dim=1)\n",
        "        cls_loss = cls_criterion(cls_scores, labels.float()) \n",
        "        cls_loss_batch.append(cls_loss)       \n",
        "        loss = cls_loss \n",
        "        if country_only or country_and_length_and_vocab:  \n",
        "            doc_embeds_country = GradientReversalFunction.apply(doc_embeds, lambda_country) \n",
        "            disc_scores = country_discriminator(doc_embeds_country)\n",
        "            disc_loss = disc_criterion(disc_scores, country_labels.argmax(dim=1))  \n",
        "            disc_loss_batch.append(disc_loss)\n",
        "            loss += disc_loss\n",
        "        if length_only or country_and_length_and_vocab: \n",
        "            doc_embeds_length = GradientReversalFunction.apply(doc_embeds, lambda_length)   \n",
        "            len_scores = len_discriminator(doc_embeds_length)\n",
        "            len_loss = len_criterion(len_scores, length.argmax(dim=1)) \n",
        "            len_loss_batch.append(len_loss)\n",
        "            loss = loss + len_loss  \n",
        "        if vocab_only or country_and_length_and_vocab: \n",
        "            doc_embeds_vocab = GradientReversalFunction.apply(doc_embeds, lambda_vocab) \n",
        "            vocab_scores = vocab_discriminator(doc_embeds_vocab)\n",
        "            #vocab_scores = vocab_scores.squeeze(dim=1)\n",
        "            vocab_scores = vocab_scores.flatten()\n",
        "            #vocab_loss = vocab_criterion(vocab_scores, vocab.float()) \n",
        "            vocab_loss = vocab_criterion(vocab_scores, vocab.float().flatten()) \n",
        "            vocab_loss_batch.append(vocab_loss)  \n",
        "            loss = loss + vocab_loss   \n",
        "\n",
        "\n",
        "        cls_preds = torch.sigmoid(cls_scores) # Sigmoid to map predictions between 0 and 1\n",
        "        cls_pred_labels = (cls_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "        cls_gold.extend(labels.cpu().detach().numpy())\n",
        "        cls_predicted.extend(cls_pred_labels.cpu().detach().numpy())\n",
        "\n",
        "        if country_only or country_and_length_and_vocab: \n",
        "            disc_preds = torch.softmax(disc_scores, dim=1) \n",
        "            disc_gold.extend(country_labels.argmax(dim=1).cpu().detach().numpy())\n",
        "            disc_predicted.extend(disc_preds.argmax(dim=1).cpu().detach().numpy())\n",
        "        \n",
        "        if length_only or country_and_length_and_vocab: \n",
        "            len_preds = torch.softmax(len_scores, dim=1) \n",
        "            len_gold.extend(length.argmax(dim=1).cpu().detach().numpy())\n",
        "            len_predicted.extend(len_preds.argmax(dim=1).cpu().detach().numpy())\n",
        "\n",
        "        if vocab_only or country_and_length_and_vocab: \n",
        "            vocab_preds = torch.sigmoid(vocab_scores) # Sigmoid to map predictions between 0 and 1\n",
        "            vocab_pred_labels = (vocab_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "            #vocab_gold.extend(vocab.cpu().detach().numpy())\n",
        "            vocab_gold.extend(vocab.flatten().cpu().detach().numpy())\n",
        "            vocab_predicted.extend(vocab_pred_labels.cpu().detach().numpy())\n",
        "\n",
        "    cls_epoch_loss = (torch.stack(cls_loss_batch , dim=0).sum(dim=0)).item()\n",
        "    if country_only or country_and_length_and_vocab: \n",
        "        disc_epoch_loss = (torch.stack(disc_loss_batch , dim=0).sum(dim=0)).item()\n",
        "    if length_only or country_and_length_and_vocab:   \n",
        "        len_epoch_loss = (torch.stack(len_loss_batch , dim=0).sum(dim=0)).item()\n",
        "    if vocab_only or country_and_length_and_vocab:   \n",
        "        vocab_epoch_loss = (torch.stack(vocab_loss_batch , dim=0).sum(dim=0)).item()\n",
        "\n",
        "\n",
        "    cls_accuracy, cls_precision, cls_recall, cls_f1 = accuracy_score(cls_gold,cls_predicted),precision_score(cls_gold, cls_predicted),recall_score(cls_gold, cls_predicted),f1_score(cls_gold, cls_predicted)\n",
        "    #print(f\"Dev Cls Statistics : Accuracy : {100.0*cls_accuracy:4.2f}%, {100.0*cls_precision:4.2f}%,{100.0*cls_recall:4.2f}%,{100.0*cls_f1:4.2f}%\")   \n",
        "\n",
        "    if country_only or country_and_length_and_vocab: \n",
        "        disc_accuracy, disc_precision, disc_recall, disc_f1 = accuracy_score(disc_gold,disc_predicted),precision_score(disc_gold, disc_predicted,average='macro'),recall_score(disc_gold, disc_predicted,average='macro'),f1_score(disc_gold,disc_predicted,average='macro')\n",
        "        #print(f\"Train Disc Statistics : Accuracy : {100.0*disc_accuracy:4.2f}%, {100.0*disc_precision:4.2f}%,{100.0*disc_recall:4.2f}%,{100.0*disc_f1:4.2f}%\")   \n",
        "    \n",
        "    if vocab_only or country_and_length_and_vocab: \n",
        "        vocab_accuracy, vocab_precision, vocab_recall, vocab_f1 = accuracy_score(vocab_gold,vocab_predicted),precision_score(vocab_gold, vocab_predicted),recall_score(vocab_gold, vocab_predicted),f1_score(vocab_gold, vocab_predicted)\n",
        "\n",
        "    if length_only or country_and_length_and_vocab: \n",
        "        len_accuracy, len_precision, len_recall, len_f1 = accuracy_score(len_gold,len_predicted),precision_score(len_gold, len_predicted,average='macro'),recall_score(len_gold, len_predicted,average='macro'),f1_score(len_gold, len_predicted,average='macro')\n",
        "\n",
        "    if country_only:\n",
        "        dev_df.loc[len(dev_df)] = [round(cls_epoch_loss,2),round(100*cls_accuracy,2), round(100*cls_precision,2), round(100*cls_recall,2), round(100*cls_f1,2),round(disc_epoch_loss,2),round(100*disc_accuracy,2), round(100*disc_precision,2), round(100*disc_recall,2), round(100*disc_f1,2),\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]\n",
        "\n",
        "    if length_only:\n",
        "        dev_df.loc[len(dev_df)] = [round(cls_epoch_loss,2),round(100*cls_accuracy,2), round(100*cls_precision,2), round(100*cls_recall,2), round(100*cls_f1,2),\"\",\"\",\"\",\"\",\"\",round(len_epoch_loss,2),round(100*len_accuracy,2), round(100*len_precision,2), round(100*len_recall,2), round(100*len_f1,2),\"\",\"\",\"\",\"\",\"\"]\n",
        "\n",
        "    if vocab_only:\n",
        "        dev_df.loc[len(dev_df)] = [round(cls_epoch_loss,2),round(100*cls_accuracy,2), round(100*cls_precision,2), round(100*cls_recall,2), round(100*cls_f1,2),\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",round(vocab_epoch_loss,2),round(100*vocab_accuracy,2), round(100*vocab_precision,2), round(100*vocab_recall,2), round(100*vocab_f1,2)]\n",
        "\n",
        "    if country_and_length_and_vocab:\n",
        "        dev_df.loc[len(dev_df)] = [round(cls_epoch_loss,2),round(100*cls_accuracy,2), round(100*cls_precision,2), round(100*cls_recall,2), round(100*cls_f1,2),round(disc_epoch_loss,2),round(100*disc_accuracy,2), round(100*disc_precision,2), round(100*disc_recall,2), round(100*disc_f1,2),round(len_epoch_loss,2),round(100*len_accuracy,2), round(100*len_precision,2), round(100*len_recall,2), round(100*len_f1,2),round(vocab_epoch_loss,2),round(100*vocab_accuracy,2), round(100*vocab_precision,2), round(100*vocab_recall,2), round(100*vocab_f1,2)]\n",
        "\n",
        "   \n",
        "    print(train_df)\n",
        "    print(dev_df)    \n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "neb3vwU6fW-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Set"
      ],
      "metadata": {
        "id": "OkaqEQ_1jwQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_file = 'bert_a_s_t_grl_src9.pth.tar'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = HANClassifierModel(device, 0.1, 768, 300, 400, 200, 100, 1,\"nlpaueb/legal-bert-base-uncased\").to(device)\n",
        "feature_extractor = model.HANmodel\n",
        "classifier = model.classifier\n",
        "\n",
        "\n",
        "checkpoint = torch.load(checkpoint_file)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "cls_predicted, cls_gold = [],[]\n",
        "    \n",
        "with torch.no_grad(): \n",
        "      for batch_idx, (docs,labels, country_labels,length,vocab_cf, doc_lengths, sent_lengths, att_mask, token_type_id) in tqdm(enumerate(test_dataloader),total=len(test_dataloader), leave=False):\n",
        "        # Get input and targets and get to cuda\n",
        "        docs, labels, country_labels,length, vocab_cf,doc_lengths, sent_lengths, att_mask, token_type_id = docs.to(device), labels.to(device),country_labels.to(device), length.to(device),vocab_cf.to(length), doc_lengths.to(device), sent_lengths.to(device), att_mask.to(device), token_type_id.to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        doc_embeds, word_att_weights, sentence_att_weights = feature_extractor(docs, doc_lengths, sent_lengths, att_mask, token_type_id)\n",
        "\n",
        "        #disc_scores = country_discriminator(doc_embeds)\n",
        "        cls_scores = classifier(doc_embeds)\n",
        "\n",
        "        cls_scores = cls_scores.squeeze(dim=1)\n",
        "        \n",
        "        cls_preds = torch.sigmoid(cls_scores) # Sigmoid to map predictions between 0 and 1\n",
        "        cls_pred_labels = (cls_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "        cls_gold.extend(labels.cpu().detach().numpy())\n",
        "        cls_predicted.extend(cls_pred_labels.cpu().detach().numpy())\n",
        "\n",
        "    cls_accuracy, cls_precision, cls_recall, cls_f1 = accuracy_score(cls_gold,cls_predicted),precision_score(cls_gold, cls_predicted),recall_score(cls_gold, cls_predicted),f1_score(cls_gold, cls_predicted)\n",
        "    print(f\"Test Cls Statistics : Accuracy : {100.0*cls_accuracy:4.2f}%, {100.0*cls_precision:4.2f}%,{100.0*cls_recall:4.2f}%,{100.0*cls_f1:4.2f}%\")    "
      ],
      "metadata": {
        "id": "rXMOOlm5jyEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IG"
      ],
      "metadata": {
        "id": "Afl1dkMDoU3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "paragraph_num_removal = True\n",
        "tokenizer_path = \"nlpaueb/legal-bert-base-uncased\"\n",
        "max_sent_len = 500\n",
        "max_doc_len = 40\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "def spl_filter(sentence, spl_tokens):\n",
        "    return [y for y in sentence if y  not in spl_tokens]\n",
        "\n",
        "def pack_sentences(token_encodings, max_sent_len, max_doc_len):\n",
        "    doc = []\n",
        "    sentence_len = [len(x) for x in token_encodings]\n",
        "    i = 0\n",
        "    sum = 0\n",
        "    each_sentence_group = []\n",
        "    mapping = []\n",
        "\n",
        "    while i <len(sentence_len):\n",
        "      start = sum\n",
        "      sum += sentence_len[i] \n",
        "      if(sentence_len[i]>max_sent_len):\n",
        "        print(\"greater than 512\")\n",
        "      if(sum<= max_sent_len):\n",
        "        each_sentence_group.extend(token_encodings[i])\n",
        "        mapping.append({\"start\": start , \"end\": sum-1 , \"pack_num\":len(doc)})\n",
        "      else:\n",
        "        doc.append(each_sentence_group)\n",
        "        each_sentence_group = token_encodings[i]\n",
        "        sum = sentence_len[i] \n",
        "        mapping.append({\"start\": 0 , \"end\": sum-1 , \"pack_num\":len(doc)})\n",
        "      i +=1\n",
        "    \n",
        "    if(len(each_sentence_group)!=0):\n",
        "      doc.append(each_sentence_group)\n",
        "\n",
        "    if(len(doc)>max_doc_len):\n",
        "      doc = doc[:max_doc_len]\n",
        "      mapping = [x for x in mapping if x[\"pack_num\"] < max_doc_len]\n",
        "\n",
        "    return doc, mapping\n",
        "\n",
        "def encode(text):\n",
        "    if paragraph_num_removal :\n",
        "      text = [re.sub(r'^\\d+\\. ', '',paragraph)  for paragraph in text]\n",
        "      \n",
        "    text_tokens = [tokenizer.encode(paragraph) for paragraph in text]\n",
        "    \n",
        "    spl_tokens = [tokenizer.sep_token_id, tokenizer.cls_token_id]\n",
        "    text_tokenized = [spl_filter(paragraph, spl_tokens) for paragraph in text_tokens]\n",
        "    \n",
        "    encoding,mapping = pack_sentences(text_tokenized, max_sent_len, max_doc_len)\n",
        "    return encoding, mapping\n"
      ],
      "metadata": {
        "id": "8fW9oe3poVzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sentence_for_batch(tokens_lists, max_len: int, pad_token_id):\n",
        "        pad_id = pad_token_id\n",
        "        toks_ids = []\n",
        "        att_masks = []\n",
        "        tok_type_lists = []\n",
        "        for item_toks in tokens_lists:\n",
        "            padded_item_toks = item_toks + [pad_id] * (max_len - len(item_toks))\n",
        "            toks_ids.append(padded_item_toks)\n",
        "\n",
        "            att_mask = [1] * len(item_toks) + [0] * (max_len - len(item_toks))\n",
        "            att_masks.append(att_mask)\n",
        "\n",
        "            tok_type_list = [0] * (max_len)\n",
        "            tok_type_lists.append(tok_type_list)\n",
        "            \n",
        "        return toks_ids, att_masks, tok_type_lists\n",
        "\n",
        "\n",
        "def batchify(encoding, pad_token_id):\n",
        "        doc_lengths = [len(encoding)]\n",
        "        sent_lengths = []\n",
        "        sent_lengths.append([len(i) for i in encoding])\n",
        "        \n",
        "        batch_sz = 1\n",
        "        batch_max_doc_length = max(doc_lengths)\n",
        "        batch_max_sent_length = max([max(sl) for sl in sent_lengths])\n",
        "\n",
        "        docs_tensor = torch.zeros((batch_sz, batch_max_doc_length, batch_max_sent_length), dtype=torch.long)\n",
        "        batch_att_mask_tensor = torch.zeros((batch_sz, batch_max_doc_length, batch_max_sent_length), dtype=torch.long)\n",
        "        token_type_ids_tensor = torch.zeros((batch_sz, batch_max_doc_length, batch_max_sent_length), dtype=torch.long)\n",
        "        sent_lengths_tensor = torch.zeros((batch_sz, batch_max_doc_length))\n",
        "\n",
        "        \n",
        "        doc_idx = 0\n",
        "        padded_token_lists, att_mask_lists, tok_type_lists = pad_sentence_for_batch(encoding, batch_max_sent_length,pad_token_id)\n",
        "        doc_length = doc_lengths[doc_idx]\n",
        "        sent_lengths_tensor[doc_idx, :doc_length] = torch.tensor(sent_lengths[doc_idx], dtype=torch.long)\n",
        "\n",
        "        for sent_idx, (padded_tokens, att_masks, tok_types) in enumerate(\n",
        "                    zip(padded_token_lists, att_mask_lists, tok_type_lists)):\n",
        "                docs_tensor[doc_idx, sent_idx, :] = torch.tensor(padded_tokens, dtype=torch.long)\n",
        "                batch_att_mask_tensor[doc_idx, sent_idx, :] = torch.tensor(att_masks, dtype=torch.long)\n",
        "                token_type_ids_tensor[doc_idx, sent_idx, :] = torch.tensor(tok_types, dtype=torch.long)\n",
        "\n",
        "        return (\n",
        "            docs_tensor,\n",
        "            torch.tensor(doc_lengths, dtype=torch.long),\n",
        "            sent_lengths_tensor,\n",
        "            batch_att_mask_tensor, \n",
        "            token_type_ids_tensor,\n",
        "        )"
      ],
      "metadata": {
        "id": "SK0E2hrAodL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dev "
      ],
      "metadata": {
        "id": "TX6ejQiLoh3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sampling an input \n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_pickle('dev.pkl')\n",
        "#text = (df.iloc[[1]]['TEXT'].to_numpy())[0]\n",
        "#print(text)"
      ],
      "metadata": {
        "id": "GN6xE3Gzoe0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ['001-4553',\n",
        "'001-57904',\n",
        "'001-58115',\n",
        "'001-58227',\n",
        "'001-61317',\n",
        "'001-67816',\n",
        "'001-69131',\n",
        "'001-73291',\n",
        "'001-76677',\n",
        "'001-77181',\n",
        "'001-80083',\n",
        "'001-83482',\n",
        "'001-90165',\n",
        "'001-88523',\n",
        "'001-97402',\n",
        "'001-91415',\n",
        "'001-90276',\n",
        "'001-102788',\n",
        "'001-102634',\n",
        "'001-102256',\n",
        "'001-101853',\n",
        "'001-107174',\n",
        "'001-108189',\n",
        "'001-108225',\n",
        "'001-106159']"
      ],
      "metadata": {
        "id": "J9U7CqP_ogO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "thxHcaKwokBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sampling an input \n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_pickle('test.pkl')\n",
        "#text = (df.iloc[[1]]['TEXT'].to_numpy())[0]\n",
        "#print(text)"
      ],
      "metadata": {
        "id": "N07SCCmJokuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ['001-140004',\n",
        "       '001-140021',\n",
        "       '001-140754',\n",
        "       '001-142196',\n",
        "       '001-144107',\n",
        "       '001-145014',\n",
        "       '001-154400',\n",
        "       '001-155374',\n",
        "       '001-158470',\n",
        "       '001-158484',\n",
        "       '001-161061',\n",
        "       '001-162023',\n",
        "       '001-164315',\n",
        "       '001-166774',\n",
        "       '001-167112',\n",
        "       '001-170052',\n",
        "       '001-170360',\n",
        "       '001-170858',\n",
        "       '001-171971',\n",
        "       '001-178750',\n",
        "       '001-180548',\n",
        "       '001-183127',\n",
        "       '001-184659',\n",
        "       '001-184674',\n",
        "       '001-185320',\n",
        "       ]"
      ],
      "metadata": {
        "id": "i0zGaEqcomE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IG generation"
      ],
      "metadata": {
        "id": "RsHGidBdpTmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IG(text):\n",
        "      encoding, mapping = encode(text)\n",
        "      docs, doc_lengths, sent_lengths, att_mask, token_type_id = batchify (encoding, tokenizer.pad_token_id)\n",
        "      docs, doc_lengths, sent_lengths, att_mask, token_type_id = docs.to(device), doc_lengths.to(device), sent_lengths.to(device), att_mask.to(device), token_type_id.to(device)\n",
        "      #print(docs.shape)\n",
        "      sents, sent_lengths_pre, attn_masks, token_types, docs_valid_bsz,  doc_perm_idx, sent_perm_idx = model.HANmodel.word_attention_pre(\n",
        "                      docs, doc_lengths, sent_lengths, att_mask, token_type_id\n",
        "                  )\n",
        "\n",
        "      reference_indices = torch.full(sents.shape, tokenizer.pad_token_id).to(device)\n",
        "\n",
        "      attributions_ig = lig.attribute(sents, baselines = reference_indices,  additional_forward_args=(sent_lengths_pre, attn_masks, token_types, docs_valid_bsz,  doc_perm_idx, sent_perm_idx), n_steps=1)\n",
        "      attributions = attributions_ig.sum(dim=-1)\n",
        "      attributions = attributions / torch.norm(attributions)\n",
        "      return attributions, encoding, mapping\n",
        "      "
      ],
      "metadata": {
        "id": "NwQQQC9sonvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "import numpy\n",
        "\n",
        "!pip install captum\n",
        "from captum.attr import visualization as viz\n",
        "from captum.attr import LayerConductance, LayerIntegratedGradients, IntegratedGradients\n",
        "from captum.attr import configure_interpretable_embedding_layer\n",
        "\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.backends.cudnn.enabled=False\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = HANClassifierModel(device, 0.1, 768, 300, 400, 200, 100, 1,\"nlpaueb/legal-bert-base-uncased\").to(device)\n",
        "\n",
        "def my_forward_func(sents, sent_lengths, attn_masks, token_types, docs_valid_bsz,  doc_perm_idx, sent_perm_idx):\n",
        "  sent_embeddings, doc_perm_idx, docs_valid_bsz, word_att_weights = model.HANmodel.word_attention_post(\n",
        "                 sents, sent_lengths, attn_masks, token_types, docs_valid_bsz,  doc_perm_idx, sent_perm_idx\n",
        "            )\n",
        "  doc_embeds, word_att_weights, sentence_att_weights = model.HANmodel.sentence_attention(\n",
        "            sent_embeddings, doc_perm_idx, docs_valid_bsz, word_att_weights\n",
        "        )\n",
        "  cls_scores = model.classifier(doc_embeds)\n",
        "  cls_scores = cls_scores.squeeze(dim=1)        \n",
        "  cls_preds = torch.sigmoid(cls_scores) \n",
        "  return cls_preds\n",
        "\n",
        "\n",
        "scores_dict = {}\n",
        "\n",
        "checkpoint_file = 'grad_cou_2.pth.tar'\n",
        "model_arch = 'grad_cou'\n",
        "checkpoint = torch.load(checkpoint_file)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "lig = LayerIntegratedGradients(my_forward_func,model.HANmodel.word_attention_post.bert_model.embeddings)\n",
        "\n",
        "for id in ids:\n",
        "          model.eval()\n",
        "          model.zero_grad()\n",
        "          text = ((df.loc[df['ITEMID'] == id]['TEXT']).to_numpy())[0]\n",
        "          attributions, encoding, mapping = IG(text)\n",
        "          scores_dict[id] = []\n",
        "          scores_dict[id].append({model_arch: {\"mapping\":mapping, \"attributions\" :attributions} })\n",
        "\n",
        "\n",
        "          gc.collect()\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "with open('final_lex/grad_cou_test.pkl', 'wb') as f:\n",
        "   pickle.dump(scores_dict, f)"
      ],
      "metadata": {
        "id": "cBMyxoIUpIVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lexglue"
      ],
      "metadata": {
        "id": "-YAQEjcpqzfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lex_b = {}\n",
        "lex_b['train'] = load_dataset(\"lex_glue\", \"ecthr_b\", split='train')\n",
        "lex_b['val'] = load_dataset(\"lex_glue\", \"ecthr_b\", split='validation')\n",
        "lex_b['test'] = load_dataset(\"lex_glue\", \"ecthr_b\", split='test')"
      ],
      "metadata": {
        "id": "6IpJbujjq1EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import torchtext\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import BertModel\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler, KBinsDiscretizer\n",
        "import torch\n",
        "import json\n",
        "\n",
        "def len_condition(each_text):\n",
        "  k = sum([1 if re.match(r'^\\d+\\. ',x) is not None else 0 for x in each_text])\n",
        "  return k if k!=0 else len(each_text)\n",
        "\n",
        "class LexGlueDataset(Dataset):\n",
        "  def __init__(self, dataset_path, bert_path, max_sent_len, max_doc_len,task_a_classes=None,task_b_classes=None,cou_classes=None,scaler=None,paragraph_num_removal = False, vocab_confounder_b =None, vocab_confounder_a = None ,vocab_confounder_ab = None):\n",
        "    self.max_sent_len = max_sent_len\n",
        "    self.max_doc_len = max_doc_len\n",
        "    self.encoding = []\n",
        "    self.labels = []\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(bert_path)\n",
        "    self.sep_token_id = self.tokenizer.sep_token_id\n",
        "    self.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "    with open(dataset_path, 'r') as json_file:\n",
        "      json_list = list(json_file)\n",
        "\n",
        "    data = []\n",
        "    for json_str in json_list:\n",
        "        data.append(json.loads(json_str))\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df = df[['facts', 'defendants','allegedly_violated_articles','violated_articles','silver_rationales', 'gold_rationales']]\n",
        "\n",
        "    task_b = {'6','3','5','P1-1','8','2','10','11','14','9'}\n",
        "    task_a = {'6','3','5','P1-1','8','2','10','11','14','9'}\n",
        "\n",
        "    \n",
        "    df['def_len'] = df['defendants'].apply(lambda x:len(x))\n",
        "    \n",
        "    df = df.loc[df['def_len'] != 0]\n",
        "    df = df.reset_index()\n",
        "\n",
        "    df['defendants'] = df['defendants'].apply(lambda x:[x[0]])\n",
        "    \n",
        "    df['len'] = df[\"facts\"].apply(lambda row: len_condition(row))\n",
        "\n",
        "    if paragraph_num_removal :\n",
        "      df[\"facts\"] = df[\"facts\"].apply(lambda row: [re.sub(r'^\\d+\\. ', '',sentence)  for sentence in row])   \n",
        "    \n",
        "    text_list = df[\"facts\"].apply(lambda row: [self.tokenizer.encode(sentence) for sentence in row])\n",
        "    spl_tokens = [self.tokenizer.sep_token_id, self.tokenizer.cls_token_id]\n",
        "    text_list = text_list.apply(lambda row:  [self.filter(sentence, spl_tokens) for sentence in row])\n",
        "    self.encoding = text_list.apply(lambda row: self.pack_sentences(row))\n",
        "\n",
        "    df['allegedly_violated_articles'] = df['allegedly_violated_articles'].apply(lambda row: [article  for article in row if article in task_b]) \n",
        "    df['violated_articles'] = df['violated_articles'].apply(lambda row: [article  for article in row if article in task_a]) \n",
        "\n",
        "    \n",
        "    if(vocab_confounder_b ):\n",
        "        final_tok_b = vocab_confounder_b\n",
        "        self.vocab_cf_b = []\n",
        "        for i in range(len(final_tok_b)):\n",
        "          self.vocab_cf_b.append(df[\"facts\"].apply(lambda x: 1 if len(set((\" \".join(x)).split()).intersection(set(final_tok_b[i])))!=0 else 0))\n",
        "        self.vocab_cf_b_zip = list(zip(*self.vocab_cf_b))\n",
        "    else:\n",
        "        self.vocab_cf_b_zip = [0]*len(self.encoding)\n",
        "    \n",
        "    if(vocab_confounder_a):\n",
        "        final_tok_a = vocab_confounder_a\n",
        "        self.vocab_cf_a = []\n",
        "        for i in range(len(final_tok_a)):\n",
        "          self.vocab_cf_a.append(df[\"facts\"].apply(lambda x: 1 if len(set((\" \".join(x)).split()).intersection(set(final_tok_a[i])))!=0 else 0))\n",
        "        self.vocab_cf_a_zip = list(zip(*self.vocab_cf_a))\n",
        "    else:\n",
        "        self.vocab_cf_a_zip = [0]*len(self.encoding)\n",
        "\n",
        "    if(vocab_confounder_ab ):\n",
        "        final_tok_ab = vocab_confounder_ab\n",
        "        self.vocab_cf_ab = []\n",
        "        for i in range(len(final_tok_ab)):\n",
        "          self.vocab_cf_ab.append(df[\"facts\"].apply(lambda x: 1 if len(set((\" \".join(x)).split()).intersection(set(final_tok_ab[i])))!=0 else 0))\n",
        "        self.vocab_cf_ab_zip = list(zip(*self.vocab_cf_ab))\n",
        "    else:\n",
        "        self.vocab_cf_ab_zip = [0]*len(self.encoding)\n",
        "    \n",
        "\n",
        "    mlb_task_b = MultiLabelBinarizer()\n",
        "    if task_b_classes is None:\n",
        "      self.task_b_labels = mlb_task_b.fit_transform(df.pop('allegedly_violated_articles'))\n",
        "    else:\n",
        "      mlb_task_b.fit([task_b_classes])\n",
        "      self.task_b_labels = mlb_task_b.transform(df.pop('allegedly_violated_articles'))\n",
        "\n",
        "    mlb_task_a = MultiLabelBinarizer()\n",
        "    if task_a_classes is None:\n",
        "      self.task_a_labels = mlb_task_a.fit_transform(df.pop('violated_articles'))\n",
        "    else:\n",
        "      mlb_task_a.fit([task_a_classes])\n",
        "      self.task_a_labels = mlb_task_a.transform(df.pop('violated_articles'))\n",
        "\n",
        "    mlb_cou = MultiLabelBinarizer()\n",
        "    if cou_classes is None:\n",
        "      self.country = mlb_cou.fit_transform(df.pop('defendants'))\n",
        "    else:\n",
        "      mlb_cou.fit([cou_classes])\n",
        "      self.country = mlb_cou.transform(df.pop('defendants'))\n",
        "    \n",
        "    self.length = df['len']\n",
        "    \n",
        "    if scaler is None:\n",
        "      self.scaler = KBinsDiscretizer(n_bins=4, encode='onehot-dense', strategy='kmeans')\n",
        "      self.length = self.scaler.fit_transform(df[['len']])\n",
        "    else:\n",
        "      self.length = scaler.transform(df[['len']])\n",
        "\n",
        "    self.silver_rationales = df['silver_rationales'].to_numpy()\n",
        "\n",
        "    self.gold_rationales = df['gold_rationales'].to_numpy()\n",
        "    \n",
        "    \n",
        "    self.task_b_classes = mlb_task_b.classes_\n",
        "    self.task_a_classes = mlb_task_a.classes_\n",
        "    self.cou_classes = mlb_cou.classes_\n",
        "    # Number of examples.\n",
        "    self.n_examples = len(self.task_a_labels)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_examples\n",
        "\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    return (self.encoding[item],  self.task_a_labels[item], self.task_b_labels[item], self.country[item], self.length[item], self.vocab_cf_a_zip[item], self.vocab_cf_b_zip[item],self.vocab_cf_ab_zip[item], self.silver_rationales[item], self.gold_rationales[item])\n",
        "  \n",
        "  def filter(self, sentence, spl_tokens):\n",
        "    return [y for y in sentence if y  not in spl_tokens]\n",
        "\n",
        "  \n",
        "  def pack_sentences(self,token_encodings):\n",
        "    doc = []\n",
        "    sentence_len = [len(x) for x in token_encodings]\n",
        "    i = 0\n",
        "    sum = 0\n",
        "    each_sentence_group = []\n",
        "    \n",
        "    while i <len(sentence_len):\n",
        "      while(sentence_len[i]> self.max_sent_len):\n",
        "        if(len(each_sentence_group)!=0):\n",
        "            doc.append(each_sentence_group)\n",
        "        doc.append(token_encodings[i][:self.max_sent_len])\n",
        "        sentence_len[i] = sentence_len[i] - self.max_sent_len\n",
        "        token_encodings[i] = token_encodings[i][self.max_sent_len:]\n",
        "        each_sentence_group = []\n",
        "        sum = 0\n",
        "      sum += sentence_len[i] \n",
        "      if(sum<= self.max_sent_len):\n",
        "        each_sentence_group.extend(token_encodings[i])\n",
        "      else:\n",
        "        doc.append(each_sentence_group)\n",
        "        each_sentence_group = token_encodings[i]\n",
        "        sum = sentence_len[i] \n",
        "      i +=1\n",
        "    \n",
        "    if(len(each_sentence_group)!=0):\n",
        "      doc.append(each_sentence_group)\n",
        "    \n",
        "    if(len(doc)>self.max_doc_len):\n",
        "      doc = doc[:self.max_doc_len]\n",
        "    return doc"
      ],
      "metadata": {
        "id": "wE04ZhrksPXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCollate:\n",
        "    def __init__(self, pad_idx, sep_idx):\n",
        "        self.pad_token_id = pad_idx\n",
        "        self.sep_token_id = sep_idx\n",
        "\n",
        "    def pad_sentence_for_batch(self, tokens_lists, max_len: int):\n",
        "        pad_id = self.pad_token_id\n",
        "        toks_ids = []\n",
        "        att_masks = []\n",
        "        tok_type_lists = []\n",
        "        for item_toks in tokens_lists:\n",
        "            padded_item_toks = item_toks + [pad_id] * (max_len - len(item_toks))\n",
        "            toks_ids.append(padded_item_toks)\n",
        "\n",
        "            att_mask = [1] * len(item_toks) + [0] * (max_len - len(item_toks))\n",
        "            att_masks.append(att_mask)\n",
        "\n",
        "            tok_type_list = [0] * (max_len)\n",
        "            tok_type_lists.append(tok_type_list)\n",
        "            \n",
        "        return toks_ids, att_masks, tok_type_lists\n",
        "\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        batch = filter(lambda x: x is not None, batch)\n",
        "        docs, task_a_labels, task_b_labels, country, length, vocab_cf_a,vocab_cf_b,vocab_cf_ab, silver, gold = list(zip(*batch))\n",
        "        doc_lengths = [len(x) for x in docs]\n",
        "        sent_lengths = []\n",
        "        for element in docs:\n",
        "          sent_lengths.append([len(i) for i in element])\n",
        "        \n",
        "        batch_sz = len(task_a_labels)\n",
        "        batch_max_doc_length = max(doc_lengths)\n",
        "        batch_max_sent_length = max([max(sl) for sl in sent_lengths])\n",
        "\n",
        "        docs_tensor = torch.zeros((batch_sz, batch_max_doc_length, batch_max_sent_length), dtype=torch.long)\n",
        "        batch_att_mask_tensor = torch.zeros((batch_sz, batch_max_doc_length, batch_max_sent_length), dtype=torch.long)\n",
        "        token_type_ids_tensor = torch.zeros((batch_sz, batch_max_doc_length, batch_max_sent_length), dtype=torch.long)\n",
        "        sent_lengths_tensor = torch.zeros((batch_sz, batch_max_doc_length))\n",
        "\n",
        "        for doc_idx, doc in enumerate(docs):\n",
        "            padded_token_lists, att_mask_lists, tok_type_lists = self.pad_sentence_for_batch(doc, batch_max_sent_length)\n",
        "\n",
        "            doc_length = doc_lengths[doc_idx]\n",
        "            sent_lengths_tensor[doc_idx, :doc_length] = torch.tensor(sent_lengths[doc_idx], dtype=torch.long)\n",
        "\n",
        "            for sent_idx, (padded_tokens, att_masks, tok_types) in enumerate(\n",
        "                    zip(padded_token_lists, att_mask_lists, tok_type_lists)):\n",
        "                docs_tensor[doc_idx, sent_idx, :] = torch.tensor(padded_tokens, dtype=torch.long)\n",
        "                batch_att_mask_tensor[doc_idx, sent_idx, :] = torch.tensor(att_masks, dtype=torch.long)\n",
        "                token_type_ids_tensor[doc_idx, sent_idx, :] = torch.tensor(tok_types, dtype=torch.long)\n",
        "\n",
        "        return (\n",
        "            docs_tensor,\n",
        "            torch.tensor(task_a_labels, dtype=torch.long),\n",
        "            torch.tensor(task_b_labels, dtype=torch.long),\n",
        "            torch.tensor(country,dtype=torch.long),\n",
        "            torch.tensor(length,dtype=torch.long),\n",
        "            torch.tensor(vocab_cf_a, dtype=torch.long),\n",
        "            torch.tensor(vocab_cf_b, dtype=torch.long),\n",
        "            torch.tensor(vocab_cf_ab, dtype=torch.long),\n",
        "            torch.tensor(doc_lengths, dtype=torch.long),\n",
        "            sent_lengths_tensor,\n",
        "            batch_att_mask_tensor, \n",
        "            token_type_ids_tensor,\n",
        "            silver,\n",
        "            gold\n",
        "        )"
      ],
      "metadata": {
        "id": "697RN5aDscZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph_num_removal = True\n",
        "bert_path = \"nlpaueb/legal-bert-base-uncased\"\n",
        "vocab_confounder_b = ['hearing', 'born', 'adjourned', 'detained', 'noted', 'hearing', 'alleged', 'investigation', 'place', 'question', 'members']\n",
        "vocab_confoudner_a =[ 'stated','could','also','one','detained','investigation','within','due','second','hearing','certain']\n",
        "vocab_confoudner_ab =[ 'february','published','november','march','religious','investigation','first','service','letter','carried','would','one','submitted','head','march','damage','group','provided','seen']\n",
        "        \n",
        "train_dataset = LexGlueDataset(lex_b['train'],bert_path = bert_path, max_sent_len = 500, max_doc_len = 40,paragraph_num_removal = paragraph_num_removal,vocab_confounder_b =vocab_confounder_b, vocab_confounder_a =vocab_confounder_a,vocab_confounder_ab =vocab_confounder_ab )\n",
        "dev_dataset = LexGlueDataset(lex_b['val'],bert_path = bert_path, max_sent_len = 500, max_doc_len = 40, classes= train_dataset.classes,paragraph_num_removal = paragraph_num_removal,vocab_confounder_b =vocab_confounder_b, vocab_confounder_a =vocab_confounder_a,vocab_confounder_ab =vocab_confounder_ab)\n",
        "test_dataset = LexGlueDataset(lex_b['test'],bert_path = bert_path, max_sent_len = 500, max_doc_len = 40, classes= train_dataset.classes,paragraph_num_removal = paragraph_num_removal,vocab_confounder_b =vocab_confounder_b, vocab_confounder_a =vocab_confounder_a,vocab_confounder_ab =vocab_confounder_ab)"
      ],
      "metadata": {
        "id": "ll1umWRisd-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader =  DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn = MyCollate(pad_idx = train_dataset.pad_token_id, sep_idx = train_dataset.pad_token_id))\n",
        "dev_dataloader =  DataLoader(dev_dataset, batch_size=16, shuffle=True, collate_fn = MyCollate(pad_idx = dev_dataset.pad_token_id, sep_idx = dev_dataset.pad_token_id))\n",
        "test_dataloader =  DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn = MyCollate(pad_idx = test_dataset.pad_token_id, sep_idx = test_dataset.pad_token_id))"
      ],
      "metadata": {
        "id": "OQHjm3c0svEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Only prediction"
      ],
      "metadata": {
        "id": "s2VTW10TwYld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def compute_metrics(gold, predicted):\n",
        "    gold = np.array(gold)\n",
        "    predicted = np.array(predicted)\n",
        "    \n",
        "    y_true = np.zeros((gold.shape[0], gold.shape[1] + 1), dtype=np.int32)\n",
        "    y_true[:, :-1] = gold\n",
        "    y_true[:, -1] = (np.sum(gold, axis=1) == 0).astype('int32')\n",
        "        # Fix predictions\n",
        "    y_pred = np.zeros((predicted.shape[0], predicted.shape[1] + 1), dtype=np.int32)\n",
        "    y_pred[:, :-1] = predicted\n",
        "    y_pred[:, -1] = (np.sum(predicted, axis=1) == 0).astype('int32')\n",
        "    \n",
        "    macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n",
        "    micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n",
        "    return {'macro-f1': macro_f1, 'micro-f1': micro_f1}"
      ],
      "metadata": {
        "id": "q2vszEeywakO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "\n",
        "num_epochs = 30\n",
        "learning_rate = 1e-4\n",
        "file_prefix = \"a_vanila_\"\n",
        "task = 'a' #(a or b or ab)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "classes_num = len(train_dataset.task_a_classes) if task=='a' else len(train_dataset.task_b_classes)\n",
        "\n",
        "model = HANClassifierModel(device, 0.1, 768, 300, 400, 200, 100, classes_num,\"nlpaueb/legal-bert-base-uncased\", task).to(device)\n",
        "\n",
        "feature_extractor = model.HANmodel\n",
        "classifier = model.classifier\n",
        "\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "  if(\"bert\" in name):\n",
        "    param.requires_grad = False\n",
        "\n",
        "optimizer = optim.Adam(list(filter(lambda p: p.requires_grad, model.parameters())), lr=learning_rate)\n",
        "cls_criterion= nn.BCEWithLogitsLoss()\n",
        "\n",
        "column_names = [\"epoch\", \"mean epoch loss\", \"macro_f1\",\"micro_f1\"]\n",
        "train_df = pd.DataFrame(columns = column_names)\n",
        "dev_df = pd.DataFrame(columns = column_names)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "    model.train()\n",
        "    cls_predicted, cls_gold,loss_batch = [],[],[]\n",
        "    for batch_idx, (docs,task_a_labels, task_b_labels, country,  length, vocab_cf_a,vocab_cf_b, vocab_cf_ab, doc_lengths, sent_lengths, att_mask, token_type_id, silver, gold) in tqdm(enumerate(train_dataloader),total=len(train_dataloader), leave=False):\n",
        "        # Get input and targets and get to cuda\n",
        "        labels = task_b_labels if task == 'b' else task_a_labels\n",
        "        docs, labels, country, length,vocab_cf_a,vocab_cf_b, vocab_cf_ab,  doc_lengths, sent_lengths, att_mask, token_type_id = docs.to(device), labels.to(device), country.to(device),length.to(device),vocab_cf_a.to(device),vocab_cf_b.to(device), vocab_cf_ab.to(device),  doc_lengths.to(device), sent_lengths.to(device), att_mask.to(device), token_type_id.to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        doc_embeds, word_att_weights, sentence_att_weights = feature_extractor(docs, doc_lengths, sent_lengths, att_mask, token_type_id)\n",
        "\n",
        "        if task == 'ab':\n",
        "          doc_embeds = torch.cat((doc_embeds, task_b_labels.to(device)), 1)\n",
        "\n",
        "        cls_scores = classifier(doc_embeds)    \n",
        "        \n",
        "        cls_loss = cls_criterion(cls_scores, labels.float())\n",
        "        \n",
        "        loss = cls_loss\n",
        "        loss_batch.append(loss)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        cls_preds = torch.sigmoid(cls_scores) # Sigmoid to map predictions between 0 and 1\n",
        "        cls_pred_labels = (cls_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "        cls_gold.extend(labels.cpu().detach().numpy())\n",
        "        cls_predicted.extend(cls_pred_labels.cpu().detach().numpy())\n",
        "\n",
        "    mean_epoch_loss = (torch.stack(loss_batch , dim=0).sum(dim=0)).item()\n",
        "\n",
        "    metrics = compute_metrics(cls_gold, cls_predicted)\n",
        "    \n",
        "    train_df.loc[len(train_df)] = [epoch,round(mean_epoch_loss,2),round(100*metrics['macro-f1'],2),round(100*metrics['micro-f1'],2)]\n",
        "\n",
        "    model_checkpoint = {'model_state_dict':model.state_dict(),'optimizer': optimizer.state_dict()}\n",
        "    torch.save(model_checkpoint,file_prefix+str(epoch)+\".pth.tar\")\n",
        "\n",
        "    \n",
        "    model.eval() \n",
        "    cls_predicted, cls_gold,loss_batch = [],[],[]\n",
        "\n",
        "    with torch.no_grad(): \n",
        "      for batch_idx, (docs,task_a_labels, task_b_labels, country,  length, vocab_cf_a,vocab_cf_b, vocab_cf_ab, doc_lengths, sent_lengths, att_mask, token_type_id, silver, gold) in tqdm(enumerate(dev_dataloader),total=len(dev_dataloader), leave=False):\n",
        "        # Get input and targets and get to cuda\n",
        "        labels = task_b_labels if task == 'b' else task_a_labels\n",
        "        docs, labels, country, length,vocab_cf_a,vocab_cf_b, vocab_cf_ab, doc_lengths, sent_lengths, att_mask, token_type_id = docs.to(device), labels.to(device), country.to(device),length.to(device),vocab_cf_a.to(device),vocab_cf_b.to(device), vocab_cf_ab.to(device),doc_lengths.to(device), sent_lengths.to(device), att_mask.to(device), token_type_id.to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        doc_embeds, word_att_weights, sentence_att_weights = feature_extractor(docs, doc_lengths, sent_lengths, att_mask, token_type_id)\n",
        "\n",
        "        if task == 'ab':\n",
        "          doc_embeds = torch.cat((doc_embeds, task_b_labels.to(device)), 1)\n",
        "\n",
        "        cls_scores = classifier(doc_embeds)\n",
        "\n",
        "        cls_loss = cls_criterion(cls_scores, labels.float())\n",
        "\n",
        "        loss = cls_loss\n",
        "        loss_batch.append(loss)\n",
        "        \n",
        "        cls_preds = torch.sigmoid(cls_scores) # Sigmoid to map predictions between 0 and 1\n",
        "        cls_pred_labels = (cls_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "        cls_gold.extend(labels.cpu().detach().numpy())\n",
        "        cls_predicted.extend(cls_pred_labels.cpu().detach().numpy())\n",
        "\n",
        "\n",
        "    mean_epoch_loss = (torch.stack(loss_batch , dim=0).sum(dim=0)).item()\n",
        "    metrics = compute_metrics(cls_gold, cls_predicted)\n",
        "\n",
        "    dev_df.loc[len(dev_df)] = [epoch,round(mean_epoch_loss,2),round(100*metrics['macro-f1'],2),round(100*metrics['micro-f1'],2)]"
      ],
      "metadata": {
        "id": "-CXX6TQpwbKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Reversal for length /vocab /country / all "
      ],
      "metadata": {
        "id": "95BlvJzV1iiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def compute_metrics(gold, predicted):\n",
        "    gold = np.array(gold)\n",
        "    predicted = np.array(predicted)\n",
        "    \n",
        "    y_true = np.zeros((gold.shape[0], gold.shape[1] + 1), dtype=np.int32)\n",
        "    y_true[:, :-1] = gold\n",
        "    y_true[:, -1] = (np.sum(gold, axis=1) == 0).astype('int32')\n",
        "        # Fix predictions\n",
        "    y_pred = np.zeros((predicted.shape[0], predicted.shape[1] + 1), dtype=np.int32)\n",
        "    y_pred[:, :-1] = predicted\n",
        "    y_pred[:, -1] = (np.sum(predicted, axis=1) == 0).astype('int32')\n",
        "    \n",
        "    macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n",
        "    micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n",
        "    return {'macro-f1': macro_f1, 'micro-f1': micro_f1}"
      ],
      "metadata": {
        "id": "-oz2s4Or1o0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
        "import itertools\n",
        "\n",
        "num_epochs = 30\n",
        "violation_learning_rate = 1e-4\n",
        "country_learning_rate = 5e-5\n",
        "length_learning_rate = 5e-5\n",
        "vocab_learning_rate = 5e-5\n",
        "gamma_country = 0.1\n",
        "gamma_length = 0.1\n",
        "gamma_vocab = 0.1\n",
        "country_only = True\n",
        "length_only = False\n",
        "vocab_only = False\n",
        "country_and_length_and_vocab = False\n",
        "file_prefix = 'a_grad_cou_'\n",
        "task = 'a'\n",
        "\n",
        "\n",
        "if task == 'b':\n",
        "  vocab_classes = 11\n",
        "elif task == 'a':\n",
        "  vocab_classes = 14\n",
        "else:\n",
        "  vocab_classes = 19\n",
        "\n",
        "assert country_only ^ length_only ^ vocab_only ^ country_and_length_and_vocab\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = HANClassifierModel(device, 0.1, 768, 300, 400, 200, 100, len(train_dataset.task_b_classes),\"nlpaueb/legal-bert-base-uncased\", task).to(device)\n",
        "\n",
        "feature_extractor = model.HANmodel\n",
        "classifier = model.classifier\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "  if(\"bert\" in name):\n",
        "    param.requires_grad = False\n",
        "\n",
        "params_dict = []\n",
        "params_dict.append({'params': list(filter(lambda p: p.requires_grad, model.parameters())), 'lr': violation_learning_rate})\n",
        "\n",
        "country_modules = []\n",
        "len_modules = []\n",
        "vocab_modules = []\n",
        "\n",
        "if country_only or country_and_length_and_vocab:\n",
        "  country_modules =  country_modules + [\n",
        "              torch.nn.Linear(400, 200), \n",
        "              nn.ReLU(), \n",
        "              nn.Dropout(0.1),\n",
        "              torch.nn.Linear(200,len(train_dataset.cou_classes))\n",
        "            ]\n",
        "  country_discriminator = nn.Sequential(*country_modules).to(device)\n",
        "  params_dict.append({'params': list(country_discriminator.parameters()), 'lr': country_learning_rate})\n",
        "\n",
        "\n",
        "if length_only or country_and_length_and_vocab:\n",
        "  len_modules =  len_modules + [\n",
        "              torch.nn.Linear(400, 200), \n",
        "              nn.ReLU(), \n",
        "              nn.Dropout(0.1),\n",
        "              torch.nn.Linear(200,4)\n",
        "            ]\n",
        "  len_discriminator = nn.Sequential(*len_modules).to(device)\n",
        "  params_dict.append({'params': list(len_discriminator.parameters()), 'lr': length_learning_rate})\n",
        "\n",
        "if vocab_only or country_and_length_and_vocab:\n",
        "  vocab_modules =  vocab_modules + [\n",
        "              torch.nn.Linear(400, 200), \n",
        "              nn.ReLU(), \n",
        "              nn.Dropout(0.1),\n",
        "              torch.nn.Linear(200,vocab_classes)\n",
        "            ]\n",
        "  vocab_discriminator = nn.Sequential(*vocab_modules).to(device)\n",
        "  params_dict.append({'params': list(vocab_discriminator.parameters()), 'lr': vocab_learning_rate})\n",
        "\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(params_dict)\n",
        "\n",
        "\n",
        "cls_criterion= nn.BCEWithLogitsLoss()\n",
        "len_criterion = nn.CrossEntropyLoss()\n",
        "vocab_criterion= nn.BCEWithLogitsLoss()\n",
        "country_criterion= nn.CrossEntropyLoss()\n",
        "\n",
        "column_names = [\"cls_loss\", \"cls_macro_f1\",\"cls_micro_f1\",\"cou_loss\", \"cou_accuracy\",\"cou_precision\",\"cou_recall\",\"cou_f1\",\"len_loss\", \"len_accuracy\",\"len_precision\",\"len_recall\",\"len_f1\",\"vocab_loss\", \"vocab_accuracy\",\"vocab_precision\",\"vocab_recall\",\"vocab_f1\"]\n",
        "train_df = pd.DataFrame(columns = column_names)\n",
        "dev_df = pd.DataFrame(columns = column_names)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "    model.train()\n",
        "    if country_only or country_and_length_and_vocab: \n",
        "        country_discriminator.train()\n",
        "    if length_only or country_and_length_and_vocab: \n",
        "        len_discriminator.train() \n",
        "    if vocab_only or country_and_length_and_vocab: \n",
        "        vocab_discriminator.train() \n",
        "\n",
        "    cls_predicted, cls_gold,cls_loss_batch = [],[],[]\n",
        "    country_predicted, country_gold,country_loss_batch = [],[],[]\n",
        "    vocab_predicted, vocab_gold,vocab_loss_batch = [],[],[]\n",
        "    len_predicted, len_gold, len_loss_batch = [],[],[]\n",
        "    current_iteration = 0\n",
        "\n",
        "    for batch_idx, (docs,task_a_labels, task_b_labels, country,  length, vocab_cf_a,vocab_cf_b,vocab_cf_ab, doc_lengths, sent_lengths, att_mask, token_type_id, silver, gold) in tqdm(enumerate(train_dataloader),total=len(train_dataloader), leave=False):\n",
        "        \n",
        "        current_iteration += 1\n",
        "        p = float(current_iteration + epoch * len(src_train_dataloader)) / num_epochs / len(src_train_dataloader)\n",
        "        lambda_country, lambda_length, lambda_vocab = get_lambda(gamma_country,p), get_lambda(gamma_length,p),get_lambda(gamma_vocab,p)     \n",
        "        \n",
        "        # Get input and targets and get to cuda\n",
        "        labels = task_b_labels if task == 'b' else task_a_labels\n",
        "\n",
        "        if task=='a':\n",
        "          vocab_cf = vocab_cf_a \n",
        "        elif task =='b':\n",
        "          vocab_cf_b = vocab_cf_b\n",
        "        else:\n",
        "          vocab_cf = vocab_cf_ab \n",
        "\n",
        "        docs, labels, country, length,vocab, doc_lengths, sent_lengths, att_mask, token_type_id = docs.to(device), labels.to(device), country.to(device),length.to(device),vocab_cf.to(device), doc_lengths.to(device), sent_lengths.to(device), att_mask.to(device), token_type_id.to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        doc_embeds, word_att_weights, sentence_att_weights, sents = feature_extractor(docs, doc_lengths, sent_lengths, att_mask, token_type_id)\n",
        "        \n",
        "        if task == 'ab':\n",
        "          doc_embeds = torch.cat((doc_embeds, task_b_labels.to(device)), 1)\n",
        "\n",
        "        cls_scores = classifier(doc_embeds)\n",
        "        cls_loss = cls_criterion(cls_scores, labels.float()) \n",
        "        cls_loss_batch.append(cls_loss)       \n",
        "        loss = cls_loss \n",
        "\n",
        "        if country_only or country_and_length_and_vocab:  \n",
        "            doc_embeds_country = GradientReversalFunction.apply(doc_embeds, lambda_country) \n",
        "            country_scores = country_discriminator(doc_embeds_country)\n",
        "            country_loss = country_criterion(country_scores, country.argmax(dim=1)) \n",
        "            country_loss_batch.append(country_loss)  \n",
        "            loss = loss + country_loss  \n",
        "        if length_only or country_and_length_and_vocab:\n",
        "            doc_embeds_length = GradientReversalFunction.apply(doc_embeds, lambda_length)     \n",
        "            len_scores = len_discriminator(doc_embeds_length)\n",
        "            len_loss = len_criterion(len_scores, length.argmax(dim=1)) \n",
        "            len_loss_batch.append(len_loss)\n",
        "            loss = loss + len_loss  \n",
        "        if vocab_only or country_and_length_and_vocab:  \n",
        "            doc_embeds_vocab = GradientReversalFunction.apply(doc_embeds, lambda_vocab) \n",
        "            vocab_scores = vocab_discriminator(doc_embeds_vocab)\n",
        "            vocab_loss = vocab_criterion(vocab_scores.flatten(), vocab.float().flatten()) \n",
        "            vocab_loss_batch.append(vocab_loss)  \n",
        "            loss = loss + vocab_loss  \n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cls_preds = torch.sigmoid(cls_scores) # Sigmoid to map predictions between 0 and 1\n",
        "        cls_pred_labels = (cls_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "        cls_gold.extend(labels.cpu().detach().numpy())\n",
        "        cls_predicted.extend(cls_pred_labels.cpu().detach().numpy())\n",
        "\n",
        "        if length_only or country_and_length_and_vocab: \n",
        "            len_preds = torch.softmax(len_scores, dim=0) \n",
        "            len_gold.extend(length.argmax(dim=1).cpu().detach().numpy())\n",
        "            len_predicted.extend(len_preds.argmax(dim=1).cpu().detach().numpy())\n",
        "\n",
        "        if vocab_only or country_and_length_and_vocab: \n",
        "            vocab_preds = torch.sigmoid(vocab_scores) # Sigmoid to map predictions between 0 and 1\n",
        "            vocab_pred_labels = (vocab_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "            vocab_gold.extend(vocab.cpu().detach().numpy())\n",
        "            vocab_predicted.extend(vocab_pred_labels.cpu().detach().numpy())\n",
        "        \n",
        "        if country_only or country_and_length_and_vocab: \n",
        "            country_preds = torch.softmax(country_scores, dim=0) \n",
        "            country_gold.extend(country.argmax(dim=1).cpu().detach().numpy())\n",
        "            country_predicted.extend(country_preds.argmax(dim=1).cpu().detach().numpy())\n",
        "\n",
        "    cls_epoch_loss = (torch.stack(cls_loss_batch , dim=0).sum(dim=0)).item()\n",
        "    if country_only or country_and_length_and_vocab:   \n",
        "        country_epoch_loss = (torch.stack(country_loss_batch , dim=0).sum(dim=0)).item()\n",
        "    if length_only or country_and_length_and_vocab:   \n",
        "        len_epoch_loss = (torch.stack(len_loss_batch , dim=0).sum(dim=0)).item()\n",
        "    if vocab_only or country_and_length_and_vocab:   \n",
        "        vocab_epoch_loss = (torch.stack(vocab_loss_batch , dim=0).sum(dim=0)).item()\n",
        "\n",
        "    metrics = compute_metrics(cls_gold, cls_predicted)\n",
        "\n",
        "    if vocab_only or country_and_length_and_vocab: \n",
        "        vocab_accuracy, vocab_precision, vocab_recall, vocab_f1 = accuracy_score(vocab_gold,vocab_predicted),precision_score(vocab_gold, vocab_predicted,average='macro'),recall_score(vocab_gold, vocab_predicted,average='macro'),f1_score(vocab_gold, vocab_predicted,average='macro')\n",
        "\n",
        "    if length_only or country_and_length_and_vocab: \n",
        "        len_accuracy, len_precision, len_recall, len_f1 = accuracy_score(len_gold,len_predicted),precision_score(len_gold, len_predicted,average='macro'),recall_score(len_gold, len_predicted,average='macro'),f1_score(len_gold, len_predicted,average='macro')\n",
        "    \n",
        "    if country_only or country_and_length_and_vocab: \n",
        "        country_accuracy, country_precision, country_recall, country_f1 = accuracy_score(country_gold,country_predicted),precision_score(country_gold, country_predicted,average='macro'),recall_score(country_gold, country_predicted,average='macro'),f1_score(country_gold, country_predicted,average='macro')\n",
        "    \n",
        "    if country_only:\n",
        "        train_df.loc[len(train_df)] = [round(cls_epoch_loss,2),round(100*metrics['macro-f1'],2), round(100*metrics['micro-f1'],2),round(country_epoch_loss,2),round(100*country_accuracy,2), round(100*country_precision,2), round(100*country_recall,2), round(100*country_f1,2),\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]\n",
        "        model_checkpoint = {'model_state_dict':model.state_dict(),'cou_state_dict': country_discriminator.state_dict(),'optimizer': optimizer.state_dict()}\n",
        "    \n",
        "    if length_only:\n",
        "        train_df.loc[len(train_df)] = [round(cls_epoch_loss,2),round(100*metrics['macro-f1'],2), round(100*metrics['micro-f1'],2),\"\",\"\",\"\",\"\",\"\",round(len_epoch_loss,2),round(100*len_accuracy,2), round(100*len_precision,2), round(100*len_recall,2), round(100*len_f1,2),\"\",\"\",\"\",\"\",\"\"]\n",
        "        model_checkpoint = {'model_state_dict':model.state_dict(),'len_state_dict': len_discriminator.state_dict(),'optimizer': optimizer.state_dict()}\n",
        "    \n",
        "    if vocab_only:\n",
        "        train_df.loc[len(train_df)] = [round(cls_epoch_loss,2), round(100*metrics['macro-f1'],2), round(100*metrics['micro-f1'],2),\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",round(vocab_epoch_loss,2),round(100*vocab_accuracy,2), round(100*vocab_precision,2), round(100*vocab_recall,2), round(100*vocab_f1,2)]\n",
        "        model_checkpoint = {'model_state_dict':model.state_dict(),'vocab_state_dict': vocab_discriminator.state_dict(),'optimizer': optimizer.state_dict()}\n",
        "    \n",
        "    if country_and_length_and_vocab:\n",
        "        train_df.loc[len(train_df)] = [round(cls_epoch_loss,2),round(100*metrics['macro-f1'],2), round(100*metrics['micro-f1'],2),round(country_epoch_loss,2),round(100*country_accuracy,2), round(100*country_precision,2), round(100*country_recall,2), round(100*country_f1,2),round(len_epoch_loss,2),round(100*len_accuracy,2), round(100*len_precision,2), round(100*len_recall,2), round(100*len_f1,2),round(vocab_epoch_loss,2),round(100*vocab_accuracy,2), round(100*vocab_precision,2), round(100*vocab_recall,2), round(100*vocab_f1,2)]\n",
        "        model_checkpoint = {'model_state_dict':model.state_dict(),'country_state_dict': country_discriminator.state_dict(),'len_state_dict': len_discriminator.state_dict(),'vocab_state_dict': vocab_discriminator.state_dict(),'optimizer': optimizer.state_dict()}\n",
        "    \n",
        "    \n",
        "    torch.save(model_checkpoint,file_prefix+str(epoch)+\".pth.tar\")\n",
        "\n",
        "    model.eval()\n",
        "    if country_only or country_and_length_and_vocab: \n",
        "        country_discriminator.eval() \n",
        "    if length_only or country_and_length_and_vocab: \n",
        "        len_discriminator.eval() \n",
        "    if vocab_only or country_and_length_and_vocab: \n",
        "        vocab_discriminator.eval() \n",
        "    cls_predicted, cls_gold,cls_loss_batch = [],[],[]\n",
        "    country_predicted, country_gold, country_loss_batch = [],[],[]\n",
        "    vocab_predicted, vocab_gold, vocab_loss_batch = [],[],[]\n",
        "    len_predicted, len_gold, len_loss_batch = [],[],[]\n",
        "    with torch.no_grad(): \n",
        "      for batch_idx, (docs,task_a_labels, task_b_labels, country,  length, vocab_cf_a,vocab_cf_b, vocab_cf_ab, doc_lengths, sent_lengths, att_mask, token_type_id, silver, gold) in tqdm(enumerate(dev_dataloader),total=len(dev_dataloader), leave=False):\n",
        "        # Get input and targets and get to cuda\n",
        "        labels = task_b_labels if task == 'b' else task_a_labels\n",
        "\n",
        "        if task=='a':\n",
        "          vocab_cf = vocab_cf_a \n",
        "        elif task =='b':\n",
        "          vocab_cf_b = vocab_cf_b\n",
        "        else:\n",
        "          vocab_cf = vocab_cf_ab \n",
        "\n",
        "        docs, labels, country, length,vocab, doc_lengths, sent_lengths, att_mask, token_type_id = docs.to(device), labels.to(device), country.to(device),length.to(device),vocab_cf.to(device), doc_lengths.to(device), sent_lengths.to(device), att_mask.to(device), token_type_id.to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        doc_embeds, word_att_weights, sentence_att_weights, sents  = feature_extractor(docs, doc_lengths, sent_lengths, att_mask, token_type_id)\n",
        "\n",
        "        if task == 'ab':\n",
        "          doc_embeds = torch.cat((doc_embeds, task_b_labels.to(device)), 1)\n",
        "        \n",
        "        cls_scores = classifier(doc_embeds)\n",
        "        cls_loss = cls_criterion(cls_scores, labels.float()) \n",
        "        cls_loss_batch.append(cls_loss)       \n",
        "        loss = cls_loss \n",
        "\n",
        "        if country_only or country_and_length_and_vocab: \n",
        "            doc_embeds_country = GradientReversalFunction.apply(doc_embeds, lambda_country) \n",
        "            country_scores = country_discriminator(doc_embeds_country)\n",
        "            country_loss = country_criterion(country_scores, country.argmax(dim=1)) \n",
        "            country_loss_batch.append(country_loss)  \n",
        "            loss = loss + country_loss  \n",
        "\n",
        "        if length_only or country_and_length_and_vocab:   \n",
        "            doc_embeds_length = GradientReversalFunction.apply(doc_embeds, lambda_length) \n",
        "            len_scores = len_discriminator(doc_embeds_length)\n",
        "            len_loss = len_criterion(len_scores, length.argmax(dim=1)) \n",
        "            len_loss_batch.append(len_loss)\n",
        "            loss = loss + len_loss  \n",
        "\n",
        "        if vocab_only or country_and_length_and_vocab: \n",
        "            doc_embeds_vocab = GradientReversalFunction.apply(doc_embeds, lambda_vocab) \n",
        "            vocab_scores = vocab_discriminator(doc_embeds_vocab)\n",
        "            vocab_loss = vocab_criterion(vocab_scores.flatten(), vocab.float().flatten()) \n",
        "            vocab_loss_batch.append(vocab_loss)  \n",
        "            loss = loss + vocab_loss  \n",
        "      \n",
        "        cls_preds = torch.sigmoid(cls_scores) # Sigmoid to map predictions between 0 and 1\n",
        "        cls_pred_labels = (cls_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "        cls_gold.extend(labels.cpu().detach().numpy())\n",
        "        cls_predicted.extend(cls_pred_labels.cpu().detach().numpy())\n",
        "\n",
        "        if length_only or country_and_length_and_vocab: \n",
        "            len_preds = torch.softmax(len_scores, dim=0) \n",
        "            len_gold.extend(length.argmax(dim=1).cpu().detach().numpy())\n",
        "            len_predicted.extend(len_preds.argmax(dim=1).cpu().detach().numpy())\n",
        "\n",
        "        if vocab_only or country_and_length_and_vocab: \n",
        "            vocab_preds = torch.sigmoid(vocab_scores) # Sigmoid to map predictions between 0 and 1\n",
        "            vocab_pred_labels = (vocab_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "            vocab_gold.extend(vocab.cpu().detach().numpy())\n",
        "            vocab_predicted.extend(vocab_pred_labels.cpu().detach().numpy())\n",
        "        \n",
        "        if country_only or country_and_length_and_vocab: \n",
        "            country_preds = torch.softmax(country_scores, dim=0) \n",
        "            country_gold.extend(country.argmax(dim=1).cpu().detach().numpy())\n",
        "            country_predicted.extend(country_preds.argmax(dim=1).cpu().detach().numpy())\n",
        "\n",
        "\n",
        "    cls_epoch_loss = (torch.stack(cls_loss_batch , dim=0).sum(dim=0)).item()\n",
        "\n",
        "    if country_only or country_and_length_and_vocab:   \n",
        "        country_epoch_loss = (torch.stack(country_loss_batch , dim=0).sum(dim=0)).item()\n",
        "    if length_only or country_and_length_and_vocab:   \n",
        "        len_epoch_loss = (torch.stack(len_loss_batch , dim=0).sum(dim=0)).item()\n",
        "    if vocab_only or country_and_length_and_vocab:   \n",
        "        vocab_epoch_loss = (torch.stack(vocab_loss_batch , dim=0).sum(dim=0)).item()\n",
        "\n",
        "    metrics = compute_metrics(cls_gold, cls_predicted)\n",
        "\n",
        "    if vocab_only or country_and_length_and_vocab: \n",
        "        vocab_accuracy, vocab_precision, vocab_recall, vocab_f1 = accuracy_score(vocab_gold,vocab_predicted),precision_score(vocab_gold, vocab_predicted,average='macro'),recall_score(vocab_gold, vocab_predicted,average='macro'),f1_score(vocab_gold, vocab_predicted,average='macro')\n",
        "\n",
        "    if length_only or country_and_length_and_vocab: \n",
        "        len_accuracy, len_precision, len_recall, len_f1 = accuracy_score(len_gold,len_predicted),precision_score(len_gold, len_predicted,average='macro'),recall_score(len_gold, len_predicted,average='macro'),f1_score(len_gold, len_predicted,average='macro')\n",
        "    \n",
        "    if country_only or country_and_length_and_vocab: \n",
        "        country_accuracy, country_precision, country_recall, country_f1 = accuracy_score(country_gold,country_predicted),precision_score(country_gold, country_predicted,average='macro'),recall_score(country_gold, country_predicted,average='macro'),f1_score(country_gold, country_predicted,average='macro')\n",
        "    \n",
        "    if country_only:\n",
        "        dev_df.loc[len(dev_df)] = [round(cls_epoch_loss,2),round(100*metrics['macro-f1'],2), round(100*metrics['micro-f1'],2),round(country_epoch_loss,2),round(100*country_accuracy,2), round(100*country_precision,2), round(100*country_recall,2), round(100*country_f1,2),\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]\n",
        "\n",
        "    if length_only:\n",
        "        dev_df.loc[len(dev_df)] = [round(cls_epoch_loss,2),round(100*metrics['macro-f1'],2), round(100*metrics['micro-f1'],2),\"\",\"\",\"\",\"\",\"\",round(len_epoch_loss,2),round(100*len_accuracy,2), round(100*len_precision,2), round(100*len_recall,2), round(100*len_f1,2),\"\",\"\",\"\",\"\",\"\"]\n",
        "\n",
        "    if vocab_only:\n",
        "        dev_df.loc[len(dev_df)] = [round(cls_epoch_loss,2), round(100*metrics['macro-f1'],2), round(100*metrics['micro-f1'],2),\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",round(vocab_epoch_loss,2),round(100*vocab_accuracy,2), round(100*vocab_precision,2), round(100*vocab_recall,2), round(100*vocab_f1,2)]\n",
        "    \n",
        "    if country_and_length_and_vocab:\n",
        "        dev_df.loc[len(dev_df)] = [round(cls_epoch_loss,2),round(100*metrics['macro-f1'],2), round(100*metrics['micro-f1'],2),round(country_epoch_loss,2),round(100*country_accuracy,2), round(100*country_precision,2), round(100*country_recall,2), round(100*country_f1,2),round(len_epoch_loss,2),round(100*len_accuracy,2), round(100*len_precision,2), round(100*len_recall,2), round(100*len_f1,2),round(vocab_epoch_loss,2),round(100*vocab_accuracy,2), round(100*vocab_precision,2), round(100*vocab_recall,2), round(100*vocab_f1,2)]\n",
        "\n",
        "    print(train_df)\n",
        "    print(dev_df)    \n"
      ],
      "metadata": {
        "id": "F_kt2ueY1qLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "t5tTF9J6GFh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_file = 'bert_a_s_t_grl_src9.pth.tar'\n",
        "task = 'a'\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = HANClassifierModel(device, 0.1, 768, 300, 400, 200, 100, 1,\"nlpaueb/legal-bert-base-uncased\", task).to(device)\n",
        "feature_extractor = model.HANmodel\n",
        "classifier = model.classifier\n",
        "\n",
        "checkpoint = torch.load(checkpoint_file)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "cls_predicted, cls_gold,cls_loss_batch = [],[],[]\n",
        "\n",
        "with torch.no_grad(): \n",
        "      for batch_idx, (docs,task_a_labels, task_b_labels, country,  length, vocab_cf_a,vocab_cf_b, vocab_cf_ab, doc_lengths, sent_lengths, att_mask, token_type_id, silver, gold) in tqdm(enumerate(dev_dataloader),total=len(dev_dataloader), leave=False):\n",
        "        \n",
        "        labels = task_b_labels if task == 'b' else task_a_labels\n",
        "        docs, labels, country, length,vocab, doc_lengths, sent_lengths, att_mask, token_type_id = docs.to(device), labels.to(device), country.to(device),length.to(device),vocab_cf.to(device), doc_lengths.to(device), sent_lengths.to(device), att_mask.to(device), token_type_id.to(device)\n",
        "\n",
        "        # Forward prop\n",
        "        doc_embeds, word_att_weights, sentence_att_weights, sents  = feature_extractor(docs, doc_lengths, sent_lengths, att_mask, token_type_id)\n",
        "\n",
        "        if task == 'ab':\n",
        "          doc_embeds = torch.cat((doc_embeds, task_b_labels.to(device)), 1)\n",
        "        \n",
        "        cls_scores = classifier(doc_embeds)      \n",
        "        cls_preds = torch.sigmoid(cls_scores) # Sigmoid to map predictions between 0 and 1\n",
        "        cls_pred_labels = (cls_preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "        cls_gold.extend(labels.cpu().detach().numpy())\n",
        "        cls_predicted.extend(cls_pred_labels.cpu().detach().numpy())\n",
        "\n",
        "    metrics = compute_metrics(cls_gold, cls_predicted)\n",
        "    print(metrics)  "
      ],
      "metadata": {
        "id": "VSjAuHCtGH0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IG"
      ],
      "metadata": {
        "id": "9gMB25JlHJ3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "\n",
        "paragraph_num_removal = True\n",
        "tokenizer_path = \"nlpaueb/legal-bert-base-uncased\"\n",
        "max_sent_len = 500\n",
        "max_doc_len = 40\n",
        "classes = ['10', '11', '14', '2', '3', '5', '6', '8', '9', 'P1-1']\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "def spl_filter(sentence, spl_tokens):\n",
        "    return [y for y in sentence if y  not in spl_tokens]\n",
        "\n",
        "def pack_sentences(token_encodings, max_sent_len, max_doc_len):\n",
        "    doc = []\n",
        "    sentence_len = [len(x) for x in token_encodings]\n",
        "    i = 0\n",
        "    sum = 0\n",
        "    each_sentence_group = []\n",
        "    mapping = []\n",
        "    flag = False\n",
        "\n",
        "    while i <len(sentence_len):\n",
        "      start_doc = len(doc)\n",
        "      while(sentence_len[i]> max_sent_len):\n",
        "        if(len(each_sentence_group)!=0):\n",
        "            doc.append(each_sentence_group)\n",
        "            if(~flag):\n",
        "              start_doc = len(doc)\n",
        "        flag = True\n",
        "        doc.append(token_encodings[i][:max_sent_len])\n",
        "        sentence_len[i] = sentence_len[i] - max_sent_len\n",
        "        token_encodings[i] = token_encodings[i][max_sent_len:]\n",
        "        each_sentence_group = []\n",
        "        sum = 0\n",
        "      start = sum\n",
        "      sum += sentence_len[i] \n",
        "      if(sum<= max_sent_len):\n",
        "        each_sentence_group.extend(token_encodings[i])\n",
        "        if(~flag):\n",
        "              mapping.append({\"start\": start , \"end\": sum-1 , \"start_doc\": start_doc,  \"end_doc\":len(doc)})\n",
        "        else:\n",
        "              mapping.append({\"start\": start , \"end\": sum-1 , \"start_doc\": start_doc+1,  \"end_doc\":len(doc)})\n",
        "      else:\n",
        "        doc.append(each_sentence_group)\n",
        "        each_sentence_group = token_encodings[i]\n",
        "        sum = sentence_len[i] \n",
        "        mapping.append({\"start\": 0 , \"end\": sum-1 ,\"start_doc\":start_doc +1, \"end_doc\":len(doc)})\n",
        "      i +=1\n",
        "    \n",
        "    if(len(each_sentence_group)!=0):\n",
        "      doc.append(each_sentence_group)\n",
        "\n",
        "    if(len(doc)>max_doc_len):\n",
        "      doc = doc[:max_doc_len]\n",
        "      mapping = [x for x in mapping if x[\"end_doc\"] < max_doc_len]\n",
        "\n",
        "    return doc, mapping\n",
        "\n",
        "def encode(text, art_id):\n",
        "    if paragraph_num_removal :\n",
        "      text = [re.sub(r'^\\d+\\. ', '',paragraph)  for paragraph in text]\n",
        "      \n",
        "    text_tokens = [tokenizer.encode(paragraph) for paragraph in text]\n",
        "    \n",
        "    spl_tokens = [tokenizer.sep_token_id, tokenizer.cls_token_id]\n",
        "    text_tokenized = [spl_filter(paragraph, spl_tokens) for paragraph in text_tokens]\n",
        "    one_hot = MultiLabelBinarizer()\n",
        "    class_ids = [[x] for x in classes]\n",
        "    one_hot.fit(class_ids)\n",
        "    b_labels = one_hot.transform([art_id])\n",
        "    \n",
        "    encoding,mapping = pack_sentences(text_tokenized, max_sent_len, max_doc_len)\n",
        "    return encoding, mapping, b_labels"
      ],
      "metadata": {
        "id": "27owGI0fHOn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "\n",
        "paragraph_num_removal = True\n",
        "tokenizer_path = \"nlpaueb/legal-bert-base-uncased\"\n",
        "max_sent_len = 500\n",
        "max_doc_len = 40\n",
        "classes = ['10', '11', '14', '2', '3', '5', '6', '8', '9', 'P1-1']\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "def spl_filter(sentence, spl_tokens):\n",
        "    return [y for y in sentence if y  not in spl_tokens]\n",
        "\n",
        "def pack_sentences(token_encodings, max_sent_len, max_doc_len):\n",
        "    doc = []\n",
        "    sentence_len = [len(x) for x in token_encodings]\n",
        "    i = 0\n",
        "    sum = 0\n",
        "    each_sentence_group = []\n",
        "    mapping = []\n",
        "    flag = False\n",
        "\n",
        "    while i <len(sentence_len):\n",
        "      start_doc = len(doc)\n",
        "      while(sentence_len[i]> max_sent_len):\n",
        "        if(len(each_sentence_group)!=0):\n",
        "            doc.append(each_sentence_group)\n",
        "            if(~flag):\n",
        "              start_doc = len(doc)\n",
        "        flag = True\n",
        "        doc.append(token_encodings[i][:max_sent_len])\n",
        "        sentence_len[i] = sentence_len[i] - max_sent_len\n",
        "        token_encodings[i] = token_encodings[i][max_sent_len:]\n",
        "        each_sentence_group = []\n",
        "        sum = 0\n",
        "      start = sum\n",
        "      sum += sentence_len[i] \n",
        "      if(sum<= max_sent_len):\n",
        "        each_sentence_group.extend(token_encodings[i])\n",
        "        if(~flag):\n",
        "              mapping.append({\"start\": start , \"end\": sum-1 , \"start_doc\": start_doc,  \"end_doc\":len(doc)})\n",
        "        else:\n",
        "              mapping.append({\"start\": start , \"end\": sum-1 , \"start_doc\": start_doc+1,  \"end_doc\":len(doc)})\n",
        "      else:\n",
        "        doc.append(each_sentence_group)\n",
        "        each_sentence_group = token_encodings[i]\n",
        "        sum = sentence_len[i] \n",
        "        mapping.append({\"start\": 0 , \"end\": sum-1 ,\"start_doc\":start_doc +1, \"end_doc\":len(doc)})\n",
        "      i +=1\n",
        "    \n",
        "    if(len(each_sentence_group)!=0):\n",
        "      doc.append(each_sentence_group)\n",
        "\n",
        "    if(len(doc)>max_doc_len):\n",
        "      doc = doc[:max_doc_len]\n",
        "      mapping = [x for x in mapping if x[\"end_doc\"] < max_doc_len]\n",
        "\n",
        "    return doc, mapping\n",
        "\n",
        "def encode(text, art_id):\n",
        "    if paragraph_num_removal :\n",
        "      text = [re.sub(r'^\\d+\\. ', '',paragraph)  for paragraph in text]\n",
        "      \n",
        "    text_tokens = [tokenizer.encode(paragraph) for paragraph in text]\n",
        "    \n",
        "    spl_tokens = [tokenizer.sep_token_id, tokenizer.cls_token_id]\n",
        "    text_tokenized = [spl_filter(paragraph, spl_tokens) for paragraph in text_tokens]\n",
        "    one_hot = MultiLabelBinarizer()\n",
        "    class_ids = [[x] for x in classes]\n",
        "    one_hot.fit(class_ids)\n",
        "    b_labels = one_hot.transform([art_id])\n",
        "    \n",
        "    encoding,mapping = pack_sentences(text_tokenized, max_sent_len, max_doc_len)\n",
        "    return encoding, mapping, b_labels"
      ],
      "metadata": {
        "id": "t-0uqNA9HVcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('test.jsonl', 'r') as json_file:\n",
        "    json_list = list(json_file)\n",
        "\n",
        "data = []\n",
        "for json_str in json_list:\n",
        "   data.append(json.loads(json_str))\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df = df[['case_id','facts', 'defendants','allegedly_violated_articles','violated_articles','silver_rationales', 'gold_rationales']]\n",
        "\n",
        "task_b = {'6','3','5','P1-1','8','2','10','11','14','9'}\n",
        "task_a = {'6','3','5','P1-1','8','2','10','11','14','9'}\n",
        "\n",
        "df['rationales_len'] = df['gold_rationales'].apply(lambda x:len(x))\n",
        "df = df.loc[df['rationales_len'] != 0]\n",
        "\n",
        "\n",
        "df['allegedly_violated_articles'] = df['allegedly_violated_articles'].apply(lambda row: [article  for article in row if article in task_b]) \n",
        "df['violated_articles'] = df['violated_articles'].apply(lambda row: [article  for article in row if article in task_a]) "
      ],
      "metadata": {
        "id": "Rfrsb_53HahS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IG(text, b_labels, task):\n",
        "      encoding, mapping, b_labels = encode(text, b_labels)\n",
        "      docs, doc_lengths, sent_lengths, att_mask, token_type_id = batchify (encoding, tokenizer.pad_token_id)\n",
        "      docs, doc_lengths, sent_lengths, att_mask, token_type_id, b_labels = docs.to(device), doc_lengths.to(device), sent_lengths.to(device), att_mask.to(device), token_type_id.to(device), torch.tensor(b_labels).to(device)\n",
        "      #print(docs.shape)\n",
        "      sents, sent_lengths_pre, attn_masks, token_types, docs_valid_bsz,  doc_perm_idx, sent_perm_idx = model.HANmodel.word_attention_pre(\n",
        "                      docs, doc_lengths, sent_lengths, att_mask, token_type_id\n",
        "                  )\n",
        "\n",
        "      reference_indices = torch.full(sents.shape, tokenizer.pad_token_id).to(device)\n",
        "\n",
        "      attributions_ig = lig.attribute(sents, baselines = reference_indices,  additional_forward_args=(sent_lengths_pre, attn_masks, token_types, docs_valid_bsz,  doc_perm_idx, sent_perm_idx, b_labels, task), n_steps=1)\n",
        "      attributions = attributions_ig.sum(dim=-1)\n",
        "      attributions = attributions / torch.norm(attributions)\n",
        "      return attributions, encoding, mapping\n",
        "      "
      ],
      "metadata": {
        "id": "B3QYWf_vHcXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "import numpy\n",
        "\n",
        "!pip install captum\n",
        "from captum.attr import visualization as viz\n",
        "from captum.attr import LayerConductance, LayerIntegratedGradients, IntegratedGradients\n",
        "from captum.attr import configure_interpretable_embedding_layer\n",
        "\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.backends.cudnn.enabled=False\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def my_forward_func(sents, sent_lengths, attn_masks, token_types, docs_valid_bsz,  doc_perm_idx, sent_perm_idx, b_labels, task):\n",
        "  sent_embeddings, doc_perm_idx, docs_valid_bsz, word_att_weights = model.HANmodel.word_attention_post(\n",
        "                 sents, sent_lengths, attn_masks, token_types, docs_valid_bsz,  doc_perm_idx, sent_perm_idx\n",
        "            )\n",
        "  doc_embeds, word_att_weights, sentence_att_weights = model.HANmodel.sentence_attention(\n",
        "            sent_embeddings, doc_perm_idx, docs_valid_bsz, word_att_weights\n",
        "        )\n",
        "  if task == 'ab':\n",
        "    doc_features = torch.cat((doc_embeds, b_labels), 1)\n",
        "  cls_scores = model.classifier(doc_features)  \n",
        "  cls_scores = cls_scores.squeeze(dim=1)        \n",
        "  cls_preds = torch.sigmoid(cls_scores) \n",
        "  return cls_preds.squeeze(0)\n",
        "\n",
        "scores_dict = {}\n",
        "\n",
        "scores_dict = {}\n",
        "checkpoint_file = 'final_lex/a_b_grad_all_'+str(i)+'.pth.tar'\n",
        "task = 'a'\n",
        "model_arch = 'a_b_grad_all'\n",
        "checkpoint = torch.load(checkpoint_file)\n",
        "\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model = HANClassifierModel(device, 0.1, 768, 300, 400, 200, 100, 10,\"nlpaueb/legal-bert-base-uncased\", task).to(device)\n",
        "\n",
        "\n",
        "lig = LayerIntegratedGradients(my_forward_func,model.HANmodel.word_attention_post.bert_model.embeddings)\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "        model.eval()\n",
        "        model.zero_grad()\n",
        "        id = row['case_id']\n",
        "        text = row['facts']       \n",
        "        b_labels =  row['allegedly_violated_articles']\n",
        "\n",
        "        attributions, encoding, mapping = IG(text, b_labels, task)\n",
        "        if id not in list(scores_dict.keys()):\n",
        "            scores_dict[id] = []\n",
        "        scores_dict[id].append({model_arch: {\"mapping\":mapping, \"attributions\" :attributions} })\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "with open('final_lex/a_b_grad_all_'+str(i)+'.pkl', 'wb') as f:\n",
        "          pickle.dump(scores_dict, f)"
      ],
      "metadata": {
        "id": "txxoE2hQHher"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}